{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras in /home/sharadv/anaconda3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.14.3)\n",
      "Requirement already satisfied: h5py in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (2.7.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.0.7)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.0.9)\n",
      "Requirement already satisfied: pyyaml in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (3.12)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.11.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import dill as pkl\n",
    "import pickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "import unittest\n",
    "import RegexTester\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbed(file):\n",
    "    start = time.time()\n",
    "    print(\"Loading Embeddings\")\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    status_every = 100000\n",
    "    for i, line in enumerate(f):\n",
    "        if i%status_every == 0:\n",
    "            print('Processing line {:,}'.format(i))\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",'{:,}'.format(len(model)),\" words loaded!\")\n",
    "    end = time.time()\n",
    "    print(\"Total Time to load embeddings:\", end - start, \"seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings\n",
      "Processing line 0\n",
      "Processing line 100,000\n",
      "Processing line 200,000\n",
      "Processing line 300,000\n",
      "Processing line 400,000\n",
      "Done. 400,001  words loaded!\n",
      "Total Time to load embeddings: 41.595529079437256 seconds\n"
     ]
    }
   ],
   "source": [
    "#TOO LARGE TO PUSH TO GIT, DOWNLOAD SEPARATLEY FROM https://github.com/stanfordnlp/GloVe\n",
    "glove_dir = './glove.6B/'\n",
    "glove_filename = 'glove.6B.300d.txt'\n",
    "glove_fullpath = glove_dir + glove_filename\n",
    "glove_dd = loadEmbed(glove_fullpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BINARIZE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pkl.load(open(\"full_data_w_flattened_plots.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(full_data[\"list_genres\"])\n",
    "full_data[\"binarized_labels\"] = y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
       "       'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Music',\n",
       "       'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Sport', 'Thriller',\n",
       "       'War', 'Western'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN/TEST SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Full Data and Generate Embeddings for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"plot_lengths\"] = full_data[\"flattened_tokens\"].apply(lambda row: len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([7.0940e+03, 1.0725e+05, 7.3764e+04, 4.0858e+04, 1.9968e+04,\n",
       "        6.4460e+03, 3.8200e+02, 4.9000e+01, 3.4000e+01, 8.0000e+00]),\n",
       " array([  4. ,  22.4,  40.8,  59.2,  77.6,  96. , 114.4, 132.8, 151.2,\n",
       "        169.6, 188. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.percentile(full_data[\"plot_lengths\"],75))\n",
    "plt.hist(full_data[\"plot_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_plot_BOW(plot):\n",
    "    word_embeddings = np.array([glove_dd.get(word, glove_dd.get(\"unk\")) for word in plot]).astype(np.float32)\n",
    "    sentence_embedding = [sum(dim) for dim in zip(*word_embeddings)]\n",
    "    return np.array(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding plots...\n",
      "Total Time to embed plots: 813.7513613700867 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding plots...\")\n",
    "start = time.time()\n",
    "full_data[\"flattened_embeddings\"] = full_data[\"flattened_tokens\"].apply(lambda row: embed_plot_BOW(row))\n",
    "end = time.time()\n",
    "print(\"Total Time to embed plots:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_pickle(\"./full_data_w_flattened_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_pickle(\"./full_data_w_flattened_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>plots</th>\n",
       "      <th>list_genres</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>flattened_tokens</th>\n",
       "      <th>binarized_labels</th>\n",
       "      <th>flattened_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>A stranded theatrical troupe manages to get ba...</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[[stranded, theatrical, troupe, manages, get, ...</td>\n",
       "      <td>[stranded, theatrical, troupe, manages, get, b...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-5.77803019399289, 0.6781772070680745, -5.014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drama</td>\n",
       "      <td>While waiting at the bus stop for the woman he...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[[wait, bus, stop, woman, intend, marry, jay, ...</td>\n",
       "      <td>[wait, bus, stop, woman, intend, marry, jay, d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-4.2481139693409204, 0.16822814429178834, -2....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    genres                                              plots list_genres  \\\n",
       "0  Comedy   A stranded theatrical troupe manages to get ba...    [Comedy]   \n",
       "1   Drama   While waiting at the bus stop for the woman he...     [Drama]   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [[stranded, theatrical, troupe, manages, get, ...   \n",
       "1  [[wait, bus, stop, woman, intend, marry, jay, ...   \n",
       "\n",
       "                                    flattened_tokens  \\\n",
       "0  [stranded, theatrical, troupe, manages, get, b...   \n",
       "1  [wait, bus, stop, woman, intend, marry, jay, d...   \n",
       "\n",
       "                                    binarized_labels  \\\n",
       "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                flattened_embeddings  \n",
       "0  [-5.77803019399289, 0.6781772070680745, -5.014...  \n",
       "1  [-4.2481139693409204, 0.16822814429178834, -2....  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_sample = full_data.iloc[0:2] #sample to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(full_data[\"flattened_embeddings\"], \n",
    "                                                    full_data[\"binarized_labels\"], test_size=0.20, random_state=42)\n",
    "#x_train = np.stack(full_data_sample[\"flattened_embeddings\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = np.stack(X_train[0:100].values)\n",
    "y_train_sample = np.stack(y_train[0:100].values)\n",
    "X_test_sample = np.stack(X_test[0:100].values)\n",
    "y_test_sample = np.stack(y_test[0:100].values)\n",
    "X_train = np.stack(X_train.values)\n",
    "y_train = np.stack(y_train.values)\n",
    "X_test = np.stack(X_test.values)\n",
    "y_test = np.stack(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "204682/204682 [==============================] - 10s 51us/step - loss: 0.2123 - acc: 0.9197\n",
      "Epoch 2/5\n",
      "204682/204682 [==============================] - 7s 37us/step - loss: 0.1977 - acc: 0.9239\n",
      "Epoch 3/5\n",
      "204682/204682 [==============================] - 7s 35us/step - loss: 0.1957 - acc: 0.9245\n",
      "Epoch 4/5\n",
      "204682/204682 [==============================] - 7s 33us/step - loss: 0.1946 - acc: 0.9248\n",
      "Epoch 5/5\n",
      "204682/204682 [==============================] - 7s 34us/step - loss: 0.1938 - acc: 0.9251\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X_train.shape[1],), activation='tanh'))\n",
    "model.add(Dense(mlb.classes_.shape[0], activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size = 100)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indiv_class_scores(y_true, y_pred, threshold, metric = None):\n",
    "    y_pred = y_pred >= threshold\n",
    "    for i in range(len(mlb.classes_)):\n",
    "        if metric == \"precision\":\n",
    "            score = precision_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        elif metric == \"recall\":\n",
    "            score = recall_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        elif metric == \"f1\":\n",
    "            score = f1_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        else:\n",
    "            return \"Not a valid metric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for Action is 0.6199095022624435\n",
      "The precision for Adventure is 0.5322164948453608\n",
      "The precision for Animation is 0.6648976497346475\n",
      "The precision for Biography is 0.6182795698924731\n",
      "The precision for Comedy is 0.6740283537467451\n",
      "The precision for Crime is 0.5676824946846208\n",
      "The precision for Drama is 0.7033793388050805\n",
      "The precision for Family is 0.6135105204872646\n",
      "The precision for Fantasy is 0.6206896551724138\n",
      "The precision for History is 0.6016528925619835\n",
      "The precision for Horror is 0.7191780821917808\n",
      "The precision for Music is 0.7645519947678221\n",
      "The precision for Musical is 0.25\n",
      "The precision for Mystery is 0.6024096385542169\n",
      "The precision for Romance is 0.5337662337662338\n",
      "The precision for Sci-Fi is 0.6857142857142857\n",
      "The precision for Sport is 0.7339667458432304\n",
      "The precision for Thriller is 0.5277227722772277\n",
      "The precision for War is 0.6545667447306791\n",
      "The precision for Western is 0.8225602027883396\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores(y_test, preds, threshold = 0.5, metric = \"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall for Action is 0.24729241877256317\n",
      "The recall for Adventure is 0.11637080867850098\n",
      "The recall for Animation is 0.2785895806861499\n",
      "The recall for Biography is 0.08556547619047619\n",
      "The recall for Comedy is 0.48663138838601866\n",
      "The recall for Crime is 0.21520687802256852\n",
      "The recall for Drama is 0.6506892895015907\n",
      "The recall for Family is 0.1350231537899098\n",
      "The recall for Fantasy is 0.07154686982444518\n",
      "The recall for History is 0.14595028067361668\n",
      "The recall for Horror is 0.270548827621747\n",
      "The recall for Music is 0.46278701504354713\n",
      "The recall for Musical is 0.0018148820326678765\n",
      "The recall for Mystery is 0.01771793054571226\n",
      "The recall for Romance is 0.07323592302209551\n",
      "The recall for Sci-Fi is 0.3747338537970192\n",
      "The recall for Sport is 0.5250637213254036\n",
      "The recall for Thriller is 0.09385455185772143\n",
      "The recall for War is 0.374916163648558\n",
      "The recall for Western is 0.5481418918918919\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores(y_true, preds, threshold=0.5, metric = \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 for Action is 0.3535483870967742\n",
      "The f1 for Adventure is 0.19098265895953756\n",
      "The f1 for Animation is 0.392657264383255\n",
      "The f1 for Biography is 0.1503267973856209\n",
      "The f1 for Comedy is 0.5652015688811612\n",
      "The f1 for Crime is 0.31209818819403856\n",
      "The f1 for Drama is 0.6760091662259827\n",
      "The f1 for Family is 0.22133439872153413\n",
      "The f1 for Fantasy is 0.1283041283041283\n",
      "The f1 for History is 0.23491448854469182\n",
      "The f1 for Horror is 0.3931847968545216\n",
      "The f1 for Music is 0.5765721331689273\n",
      "The f1 for Musical is 0.0036036036036036032\n",
      "The f1 for Mystery is 0.03442340791738382\n",
      "The f1 for Romance is 0.1287997492948919\n",
      "The f1 for Sci-Fi is 0.48462597521798995\n",
      "The f1 for Sport is 0.6121842496285289\n",
      "The f1 for Thriller is 0.15936612348632082\n",
      "The f1 for War is 0.47675906183368866\n",
      "The f1 for Western is 0.6578813988849468\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores(y_true, preds, threshold= 0.5, metric= \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = preds >= 0.5\n",
    "micro_precision = precision_score(y_true, y_pred, average = 'micro')\n",
    "weighted_macro_precision = precision_score(y_true, y_pred, average = 'weighted')\n",
    "micro_recall = recall_score(y_true, y_pred, average = 'micro')\n",
    "weighted_macro_recall = recall_score(y_true, y_pred, average = 'weighted')\n",
    "micro_f1 = f1_score(y_true, y_pred, average = 'micro')\n",
    "weighted_macro_f1 = f1_score(y_true, y_pred, average = 'weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The micro precision is 0.6787603000860902\n",
      "The weighted macro precision is 0.6433844455202036\n",
      "The micro recall is 0.3525091017479614\n",
      "The weighted macro recall is 0.3525091017479614\n",
      "The micro f1 is 0.4640284745382697\n",
      "The weighted macro f1 is 0.4201221196072813\n"
     ]
    }
   ],
   "source": [
    "print(\"The micro precision is\", micro_precision)\n",
    "print(\"The weighted macro precision is\", weighted_macro_precision)\n",
    "print(\"The micro recall is\", micro_recall)\n",
    "print(\"The weighted macro recall is\", weighted_macro_recall)\n",
    "print(\"The micro f1 is\", micro_f1)\n",
    "print(\"The weighted macro f1 is\", weighted_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dissatisfied', 'ranch', 'hand', 'become', 'bounty', 'hunter', '.'],\n",
       " ['conspires',\n",
       "  'crook',\n",
       "  'town',\n",
       "  'bos',\n",
       "  'dirty',\n",
       "  'neighboring',\n",
       "  'village',\n",
       "  'valuable',\n",
       "  'railroad',\n",
       "  'franchise',\n",
       "  'head',\n",
       "  'order',\n",
       "  'divert',\n",
       "  'town',\n",
       "  'bos',\n",
       "  'own',\n",
       "  '.'],\n",
       " ['find',\n",
       "  'former',\n",
       "  'fianc√©e',\n",
       "  'marry',\n",
       "  'sheriff',\n",
       "  'town',\n",
       "  'seek',\n",
       "  'destroy',\n",
       "  '.']]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data[\"tokenized_words\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
