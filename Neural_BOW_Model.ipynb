{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras in /home/sharadv/anaconda3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.14.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.0.9)\n",
      "Requirement already satisfied: h5py in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (2.7.1)\n",
      "Requirement already satisfied: pyyaml in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (3.12)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.0.7)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/sharadv/anaconda3/lib/python3.6/site-packages (from Keras) (1.11.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import dill as pkl\n",
    "import pickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "import unittest\n",
    "import RegexTester\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, average_precision_score, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbed(file):\n",
    "    start = time.time()\n",
    "    print(\"Loading Embeddings\")\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    status_every = 100000\n",
    "    for i, line in enumerate(f):\n",
    "        if i%status_every == 0:\n",
    "            print('Processing line {:,}'.format(i))\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",'{:,}'.format(len(model)),\" words loaded!\")\n",
    "    end = time.time()\n",
    "    print(\"Total Time to load embeddings:\", end - start, \"seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings\n",
      "Processing line 0\n",
      "Processing line 100,000\n",
      "Processing line 200,000\n",
      "Processing line 300,000\n",
      "Processing line 400,000\n",
      "Done. 400,001  words loaded!\n",
      "Total Time to load embeddings: 42.814167976379395 seconds\n"
     ]
    }
   ],
   "source": [
    "#TOO LARGE TO PUSH TO GIT, DOWNLOAD SEPARATLEY FROM https://github.com/stanfordnlp/GloVe\n",
    "glove_dir = './glove.6B/'\n",
    "glove_filename = 'glove.6B.300d.txt'\n",
    "glove_fullpath = glove_dir + glove_filename\n",
    "glove_dd = loadEmbed(glove_fullpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BINARIZE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pkl.load(open(\"full_data_w_flattened_plots.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(full_data[\"list_genres\"])\n",
    "full_data[\"binarized_labels\"] = y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
       "       'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Music',\n",
       "       'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Sport', 'Thriller',\n",
       "       'War', 'Western'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN/TEST SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Full Data and Generate Embeddings for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"plot_lengths\"] = full_data[\"flattened_tokens\"].apply(lambda row: len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([7.0940e+03, 1.0725e+05, 7.3764e+04, 4.0858e+04, 1.9968e+04,\n",
       "        6.4460e+03, 3.8200e+02, 4.9000e+01, 3.4000e+01, 8.0000e+00]),\n",
       " array([  4. ,  22.4,  40.8,  59.2,  77.6,  96. , 114.4, 132.8, 151.2,\n",
       "        169.6, 188. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.percentile(full_data[\"plot_lengths\"],75))\n",
    "plt.hist(full_data[\"plot_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_plot_BOW(plot):\n",
    "    word_embeddings = np.array([glove_dd.get(word, glove_dd.get(\"unk\")) for word in plot]).astype(np.float32)\n",
    "    sentence_embedding = [sum(dim) for dim in zip(*word_embeddings)]\n",
    "    return np.array(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding plots...\n",
      "Total Time to embed plots: 813.7513613700867 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding plots...\")\n",
    "start = time.time()\n",
    "full_data[\"flattened_embeddings\"] = full_data[\"flattened_tokens\"].apply(lambda row: embed_plot_BOW(row))\n",
    "end = time.time()\n",
    "print(\"Total Time to embed plots:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_pickle(\"./full_data_w_flattened_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_pickle(\"./full_data_w_flattened_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_sample = full_data.iloc[0:2] #sample to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(full_data[\"flattened_embeddings\"], \n",
    "                                                    full_data[\"binarized_labels\"], test_size=0.20, random_state=42)\n",
    "#x_train = np.stack(full_data_sample[\"flattened_embeddings\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = np.stack(X_train[0:100].values)\n",
    "y_train_sample = np.stack(y_train[0:100].values)\n",
    "X_test_sample = np.stack(X_test[0:100].values)\n",
    "y_test_sample = np.stack(y_test[0:100].values)\n",
    "X_train = np.stack(X_train.values)\n",
    "y_train = np.stack(y_train.values)\n",
    "X_test = np.stack(X_test.values)\n",
    "y_test = np.stack(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "204682/204682 [==============================] - 8s 41us/step - loss: 0.2120 - acc: 0.9198\n",
      "Epoch 2/5\n",
      "204682/204682 [==============================] - 7s 33us/step - loss: 0.1981 - acc: 0.9236\n",
      "Epoch 3/5\n",
      "204682/204682 [==============================] - 7s 34us/step - loss: 0.1961 - acc: 0.9242\n",
      "Epoch 4/5\n",
      "204682/204682 [==============================] - 7s 33us/step - loss: 0.1951 - acc: 0.9245\n",
      "Epoch 5/5\n",
      "204682/204682 [==============================] - 7s 33us/step - loss: 0.1943 - acc: 0.9248\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X_train.shape[1],), activation='tanh'))\n",
    "model.add(Dense(mlb.classes_.shape[0], activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size = 100)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indiv_class_scores(y_true, y_pred, threshold, metric = None):\n",
    "    y_pred = y_pred >= threshold\n",
    "    for i in range(len(mlb.classes_)):\n",
    "        if metric == \"precision\":\n",
    "            score = precision_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        elif metric == \"recall\":\n",
    "            score = recall_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        elif metric == \"f1\":\n",
    "            score = f1_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        else:\n",
    "            return \"Not a valid metric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for Action is 0.6633922724296005\n",
      "The precision for Adventure is 0.555921052631579\n",
      "The precision for Animation is 0.655680224403927\n",
      "The precision for Biography is 0.5270588235294118\n",
      "The precision for Comedy is 0.7056626506024096\n",
      "The precision for Crime is 0.5850540806293019\n",
      "The precision for Drama is 0.6837205329689101\n",
      "The precision for Family is 0.5704162976085031\n",
      "The precision for Fantasy is 0.5975609756097561\n",
      "The precision for History is 0.5931372549019608\n",
      "The precision for Horror is 0.658252427184466\n",
      "The precision for Music is 0.7526813880126183\n",
      "The precision for Musical is 0.4\n",
      "The precision for Mystery is 0.5371900826446281\n",
      "The precision for Romance is 0.5340909090909091\n",
      "The precision for Sci-Fi is 0.7271986970684039\n",
      "The precision for Sport is 0.7093922651933702\n",
      "The precision for Thriller is 0.5267441860465116\n",
      "The precision for War is 0.6560283687943262\n",
      "The precision for Western is 0.8284457478005866\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores(y_test, preds, threshold = 0.5, metric = \"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall for Action is 0.2031688728439631\n",
      "The recall for Adventure is 0.09523809523809523\n",
      "The recall for Animation is 0.29701397712833544\n",
      "The recall for Biography is 0.16666666666666666\n",
      "The recall for Comedy is 0.40781228241192036\n",
      "The recall for Crime is 0.1598602901665771\n",
      "The recall for Drama is 0.6791092258748674\n",
      "The recall for Family is 0.15695832317816233\n",
      "The recall for Fantasy is 0.09738323948327261\n",
      "The recall for History is 0.19406575781876503\n",
      "The recall for Horror is 0.3493944859572275\n",
      "The recall for Music is 0.47228820269200317\n",
      "The recall for Musical is 0.007259528130671506\n",
      "The recall for Mystery is 0.02303330970942594\n",
      "The recall for Romance is 0.06699928724162509\n",
      "The recall for Sci-Fi is 0.31689141234918383\n",
      "The recall for Sport is 0.5454545454545454\n",
      "The recall for Thriller is 0.0797675647120972\n",
      "The recall for War is 0.3722334004024145\n",
      "The recall for Western is 0.47719594594594594\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores(y_test, preds, threshold=0.5, metric = \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 for Action is 0.3110701673575925\n",
      "The f1 for Adventure is 0.1626172720712052\n",
      "The f1 for Animation is 0.4088325317009182\n",
      "The f1 for Biography is 0.2532504239683437\n",
      "The f1 for Comedy is 0.5169005383461301\n",
      "The f1 for Crime is 0.25110782865583453\n",
      "The f1 for Drama is 0.6814070779510969\n",
      "The f1 for Family is 0.24617737003058107\n",
      "The f1 for Fantasy is 0.16747365422956423\n",
      "The f1 for History is 0.2924471299093656\n",
      "The f1 for Horror is 0.4564888065982158\n",
      "The f1 for Music is 0.5803940647044515\n",
      "The f1 for Musical is 0.014260249554367202\n",
      "The f1 for Mystery is 0.04417261297995243\n",
      "The f1 for Romance is 0.11906269791006965\n",
      "The f1 for Sci-Fi is 0.4414236282748394\n",
      "The f1 for Sport is 0.6167146974063401\n",
      "The f1 for Thriller is 0.13855329561094967\n",
      "The f1 for War is 0.47496790757381263\n",
      "The f1 for Western is 0.6055734190782422\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores(y_test, preds, threshold= 0.5, metric= \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = preds >= 0.5\n",
    "micro_precision = precision_score(y_test, y_pred, average = 'micro')\n",
    "weighted_macro_precision = precision_score(y_test, y_pred, average = 'weighted')\n",
    "micro_recall = recall_score(y_test, y_pred, average = 'micro')\n",
    "weighted_macro_recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "micro_f1 = f1_score(y_test, y_pred, average = 'micro')\n",
    "weighted_macro_f1 = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The micro precision is 0.6734441883437603\n",
      "The weighted macro precision is 0.639366607023048\n",
      "The micro recall is 0.3483574272392429\n",
      "The weighted macro recall is 0.3483574272392429\n",
      "The micro f1 is 0.4591875394653757\n",
      "The weighted macro f1 is 0.4150769928178064\n"
     ]
    }
   ],
   "source": [
    "print(\"The micro precision is\", micro_precision)\n",
    "print(\"The weighted macro precision is\", weighted_macro_precision)\n",
    "print(\"The micro recall is\", micro_recall)\n",
    "print(\"The weighted macro recall is\", weighted_macro_recall)\n",
    "print(\"The micro f1 is\", micro_f1)\n",
    "print(\"The weighted macro f1 is\", weighted_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for Action is 0.4599067366508959\n",
      "The AUC for Adventure is 0.3219452169063953\n",
      "The AUC for Animation is 0.48377222318327573\n",
      "The AUC for Biography is 0.34760880021441\n",
      "The AUC for Comedy is 0.6452416672623256\n",
      "The AUC for Crime is 0.4027341374787303\n",
      "The AUC for Drama is 0.738058475816129\n",
      "The AUC for Family is 0.35154763725508303\n",
      "The AUC for Fantasy is 0.30526954299231784\n",
      "The AUC for History is 0.3934158674640972\n",
      "The AUC for Horror is 0.5200767292522198\n",
      "The AUC for Music is 0.6335010350595882\n",
      "The AUC for Musical is 0.15367840469186295\n",
      "The AUC for Mystery is 0.2269250348215422\n",
      "The AUC for Romance is 0.33037955917285006\n",
      "The AUC for Sci-Fi is 0.5300184970132945\n",
      "The AUC for Sport is 0.6357000721989505\n",
      "The AUC for Thriller is 0.36112360153748146\n",
      "The AUC for War is 0.5289629444706717\n",
      "The AUC for Western is 0.677609494145572\n",
      "The micro-avg AUC is 0.5561358610698006\n",
      "The macro-avg AUC is 0.45237378387938476\n"
     ]
    }
   ],
   "source": [
    "def auc_pr(y_true, y_pred):\n",
    "    scores = []\n",
    "    for i in range(len(mlb.classes_)):\n",
    "        p, r, d = precision_recall_curve(y_true[:,i], y_pred[:,i])\n",
    "        score = auc(r,p)\n",
    "        scores.append(score)\n",
    "        print(\"The AUC for {} is {}\".format(mlb.classes_[i], score))\n",
    "    macro_score = np.mean(scores)\n",
    "    y_true_raveled = np.ravel(y_true)\n",
    "    y_pred_raveled = np.ravel(y_pred)\n",
    "    p, r, d = precision_recall_curve(y_true_raveled, y_pred_raveled)\n",
    "    micro_score = auc(r,p)\n",
    "    print(\"The micro-avg AUC is {}\".format(micro_score))\n",
    "    print(\"The macro-avg AUC is {}\".format(macro_score))\n",
    "\n",
    "auc_pr(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BI_DIRECTIONAL LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.load(\"embed_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 80000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SENT_LENGTH = 15\n",
    "REG_PARAM = 1e-13\n",
    "l2_reg = regularizers.l2(REG_PARAM)\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=75,\n",
    "                            embeddings_regularizer=l2_reg,\n",
    "                            mask_zero = True, #determines whether masking is performed, i.e. whether the layers ignore the padded zeros in shorter documents\n",
    "                            trainable=False) \n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS+1, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(full_data['flattened_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTS = 5\n",
    "data = np.zeros((len(full_data), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "doc_lst = []\n",
    "\n",
    "# keep the MAX_NB_WORDS most frequent words and replace the rest with 'UNK'\n",
    "# truncate to the first MAX_SENTS sentences per doc and MAX_SENT_LENGTH words per sentence\n",
    "\n",
    "for summary_num, row in full_data.iterrows():\n",
    "    for sent_num, sent in enumerate(row['tokenized_words']):\n",
    "        if sent_num < MAX_SENTS:\n",
    "            #wordTokens = text_to_word_sequence(sent)\n",
    "            word_num = 0\n",
    "            words_in_sent = []\n",
    "            for _, word in enumerate(sent):\n",
    "                if word_num < MAX_SENT_LENGTH: \n",
    "                    if (word in tokenizer.word_index) and (tokenizer.word_index[word] <= MAX_NB_WORDS):\n",
    "                        data[summary_num, sent_num, word_num] = tokenizer.word_index[word]\n",
    "                        words_in_sent.append(word)\n",
    "                    else:\n",
    "                        data[summary_num, sent_num, word_num] = MAX_NB_WORDS\n",
    "                        words_in_sent.append('UNK')\n",
    "                    word_num = word_num + 1\n",
    "            doc_lst.append(words_in_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_plot_data = data.reshape(len(full_data), -1) #collapse sentence length and max sents into single dimension\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(lstm_plot_data, \n",
    "                                                    full_data[\"binarized_labels\"], test_size=0.20, random_state=42)\n",
    "\n",
    "# X_train_lstm = np.stack(X_train_lstm.values)\n",
    "y_train_lstm = np.stack(y_train_lstm.values)\n",
    "# X_test_lstm = np.stack(X_test_lstm.values)\n",
    "y_test_lstm = np.stack(y_test_lstm.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "204682/204682 [==============================] - 1319s 6ms/step - loss: 0.2538 - acc: 0.9111\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(X_train_lstm.shape[1],), dtype='int32')\n",
    "embedded_plot = embedding_layer(input_layer)\n",
    "bilstm_layer = Bidirectional(LSTM(100))(embedded_plot)\n",
    "output_layer = Dense(units=mlb.classes_.shape[0], activation=\"sigmoid\")(bilstm_layer)\n",
    "LSTM_model = Model(inputs=input_layer, outputs= output_layer)\n",
    "LSTM_model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [\"accuracy\"])\n",
    "LSTM_model.fit(X_train_lstm, y_train_lstm, epochs = 1, batch_size = 100)\n",
    "preds_LSTM = LSTM_model.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for Action is 0.6\n",
      "The precision for Adventure is 0.5\n",
      "The precision for Animation is 0.7567567567567568\n",
      "The precision for Biography is 0.5192307692307693\n",
      "The precision for Comedy is 0.6819727891156463\n",
      "The precision for Crime is 0.4148936170212766\n",
      "The precision for Drama is 0.6397024397856734\n",
      "The precision for Family is 0.0\n",
      "The precision for Fantasy is 0.0\n",
      "The precision for History is 0.5517241379310345\n",
      "The precision for Horror is 1.0\n",
      "The precision for Music is 0.7223719676549866\n",
      "The precision for Musical is 0.0\n",
      "The precision for Mystery is 0.0\n",
      "The precision for Romance is 0.5348837209302325\n",
      "The precision for Sci-Fi is 0.7784810126582279\n",
      "The precision for Sport is 0.6666666666666666\n",
      "The precision for Thriller is 0.5586206896551724\n",
      "The precision for War is 0.5291479820627802\n",
      "The precision for Western is 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharadv/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores(y_test_lstm, preds_LSTM, threshold = 0.5, metric = \"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall for Action is 0.009626955475330927\n",
      "The recall for Adventure is 0.0005635390250774866\n",
      "The recall for Animation is 0.017789072426937738\n",
      "The recall for Biography is 0.010044642857142858\n",
      "The recall for Comedy is 0.13960451190641973\n",
      "The recall for Crime is 0.01047823750671682\n",
      "The recall for Drama is 0.5216118769883351\n",
      "The recall for Family is 0.0\n",
      "The recall for Fantasy is 0.0\n",
      "The recall for History is 0.006415396952686447\n",
      "The recall for Horror is 0.0018036588508116465\n",
      "The recall for Music is 0.3182897862232779\n",
      "The recall for Musical is 0.0\n",
      "The recall for Mystery is 0.0\n",
      "The recall for Romance is 0.02459016393442623\n",
      "The recall for Sci-Fi is 0.043647977288857345\n",
      "The recall for Sport is 0.0016992353440951572\n",
      "The recall for Thriller is 0.014263074484944533\n",
      "The recall for War is 0.07914151576123407\n",
      "The recall for Western is 0.0\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores(y_test_lstm, preds_LSTM, threshold = 0.5, metric = \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 for Action is 0.0189498618239242\n",
      "The f1 for Adventure is 0.001125809175344779\n",
      "The f1 for Animation is 0.03476101800124147\n",
      "The f1 for Biography is 0.01970802919708029\n",
      "The f1 for Comedy is 0.2317651138596694\n",
      "The f1 for Crime is 0.020440251572327043\n",
      "The f1 for Drama is 0.5746530211692135\n",
      "The f1 for Family is 0.0\n",
      "The f1 for Fantasy is 0.0\n",
      "The f1 for History is 0.012683313515655966\n",
      "The f1 for Horror is 0.0036008230452674894\n",
      "The f1 for Music is 0.4418796372629843\n",
      "The f1 for Musical is 0.0\n",
      "The f1 for Mystery is 0.0\n",
      "The f1 for Romance is 0.047018739352640546\n",
      "The f1 for Sci-Fi is 0.08266129032258064\n",
      "The f1 for Sport is 0.0033898305084745766\n",
      "The f1 for Thriller is 0.027815934065934064\n",
      "The f1 for War is 0.13768961493582263\n",
      "The f1 for Western is 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharadv/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores(y_test_lstm, preds_LSTM, threshold = 0.5, metric = \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharadv/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sharadv/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred_LSTM = preds_LSTM >= 0.5\n",
    "micro_precision = precision_score(y_test_lstm, y_pred_LSTM, average = 'micro')\n",
    "weighted_macro_precision = precision_score(y_test_lstm, y_pred_LSTM, average = 'weighted')\n",
    "micro_recall = recall_score(y_test_lstm, y_pred_LSTM, average = 'micro')\n",
    "weighted_macro_recall = recall_score(y_test_lstm, y_pred_LSTM, average = 'weighted')\n",
    "micro_f1 = f1_score(y_test_lstm, y_pred_LSTM, average = 'micro')\n",
    "weighted_macro_f1 = f1_score(y_test_lstm, y_pred_LSTM, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The micro precision is 0.6459451706757365\n",
      "The weighted macro precision is 0.5534187077225805\n",
      "The micro recall is 0.16780216738699993\n",
      "The weighted macro recall is 0.16780216738699993\n",
      "The micro f1 is 0.2663996417133538\n",
      "The weighted macro f1 is 0.2048039333371301\n"
     ]
    }
   ],
   "source": [
    "print(\"The micro precision is\", micro_precision)\n",
    "print(\"The weighted macro precision is\", weighted_macro_precision)\n",
    "print(\"The micro recall is\", micro_recall)\n",
    "print(\"The weighted macro recall is\", weighted_macro_recall)\n",
    "print(\"The micro f1 is\", micro_f1)\n",
    "print(\"The weighted macro f1 is\", weighted_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
