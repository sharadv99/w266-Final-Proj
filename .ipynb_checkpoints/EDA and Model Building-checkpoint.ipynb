{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhil.patel1896/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.callbacks import History, CSVLogger\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, average_precision_score, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in train and test txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genres = open(\"train_genres.txt\", encoding=\"utf-8\").read().split('\\n')\n",
    "train_plots = open(\"train_plots.txt\", encoding=\"utf-8\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({\"genres\": train_genres, \"plots\": train_plots})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204682"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adventure', 'Animation', 'Comedy', '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[4][\"genres\"].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genres = open(\"test_genres.txt\", encoding=\"utf-8\").read().split('\\n')\n",
    "test_plots = open(\"test_plots.txt\", encoding=\"utf-8\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({\"genres\": test_genres, \"plots\": test_plots})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51171"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat dataframes\n",
    "full_data = pd.concat([train_data, test_data], ignore_index= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRIEF EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert genres to list for easier analysis\n",
    "def list_genres(row):\n",
    "    return row[\"genres\"].split(\" \")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"list_genres\"] = full_data.apply(lambda row: list_genres(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count num of movies per genre\n",
    "def dict_count(row):\n",
    "    global count_dict\n",
    "    for genre in row[\"list_genres\"]:\n",
    "        count_dict[genre] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN ONLY ONCE!!!\n",
    "count_val_series = full_data.apply(lambda row: dict_count(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.88% of the movies are Comedy\n",
      "45.84% of the movies are Drama\n",
      "2.37% of the movies are Western\n",
      "7.00% of the movies are Adventure\n",
      "6.21% of the movies are Animation\n",
      "9.71% of the movies are Action\n",
      "11.18% of the movies are Thriller\n",
      "8.08% of the movies are Family\n",
      "11.01% of the movies are Romance\n",
      "5.89% of the movies are Fantasy\n",
      "7.77% of the movies are Horror\n",
      "4.77% of the movies are History\n",
      "4.95% of the movies are Music\n",
      "5.60% of the movies are Sci-Fi\n",
      "2.85% of the movies are War\n",
      "7.27% of the movies are Crime\n",
      "2.23% of the movies are Musical\n",
      "5.18% of the movies are Biography\n",
      "5.40% of the movies are Mystery\n",
      "2.37% of the movies are Sport\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0,0,'Comedy'),\n",
       " Text(0,0,'Drama'),\n",
       " Text(0,0,'Western'),\n",
       " Text(0,0,'Adventure'),\n",
       " Text(0,0,'Animation'),\n",
       " Text(0,0,'Action'),\n",
       " Text(0,0,'Thriller'),\n",
       " Text(0,0,'Family'),\n",
       " Text(0,0,'Romance'),\n",
       " Text(0,0,'Fantasy'),\n",
       " Text(0,0,'Horror'),\n",
       " Text(0,0,'History'),\n",
       " Text(0,0,'Music'),\n",
       " Text(0,0,'Sci-Fi'),\n",
       " Text(0,0,'War'),\n",
       " Text(0,0,'Crime'),\n",
       " Text(0,0,'Musical'),\n",
       " Text(0,0,'Biography'),\n",
       " Text(0,0,'Mystery'),\n",
       " Text(0,0,'Sport')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Count num of movies per genre\n",
    "for key,val in count_dict.items():\n",
    "    print(\"{:0.2f}% of the movies are {}\".format(100*val/len(full_data), key))\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1) \n",
    "ax.bar(range(len(count_dict.keys())), list(count_dict.values()))\n",
    "ax.set_xticks(range(len(count_dict.keys())))\n",
    "ax.set_xticklabels(list(count_dict.keys()), rotation='vertical', fontsize=18)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg num of genres: 1.835858872086706\n",
      "Median num of genres: 1.0\n",
      "Max number of genres: 11\n",
      "Min number of genres: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.29306e+05, 6.44310e+04, 4.38940e+04, 1.29500e+04, 3.96000e+03,\n",
       "        1.01100e+03, 2.36000e+02, 5.20000e+01, 6.00000e+00, 7.00000e+00]),\n",
       " array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Number of genres per movie\n",
    "genre_labels = full_data[\"list_genres\"].str.len()\n",
    "print(\"Avg num of genres:\", np.mean(genre_labels))\n",
    "print(\"Median num of genres:\", np.median(genre_labels))\n",
    "print(\"Max number of genres:\", np.max(genre_labels))\n",
    "print(\"Min number of genres:\", np.min(genre_labels))\n",
    "\n",
    "\n",
    "plt.hist(genre_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of characters in a summary is 458.48165446888345, max is 1192\n"
     ]
    }
   ],
   "source": [
    "#average length of plot summary (characters)\n",
    "avg_num_chars = train_data[\"plots\"].str.len().mean()\n",
    "max_num_chars = train_data[\"plots\"].str.len().max()\n",
    "print(f'The average number of characters in a summary is {avg_num_chars}, max is {max_num_chars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of words in a summary is 79.8617855991245, max is 195\n",
      "The average number of sentences in a summary is 4.39193969181462, max is 74\n",
      "18.18371635383909\n"
     ]
    }
   ],
   "source": [
    "avg_num_words = train_data[\"plots\"].str.count(\" \").mean() + 1\n",
    "max_num_words = train_data[\"plots\"].str.count(\" \").max() + 1\n",
    "avg_num_periods = train_data[\"plots\"].str.count(\"[\\.\\?!]\").mean()\n",
    "max_num_periods = train_data[\"plots\"].str.count(\"[\\.\\?!]\").max()\n",
    "print(f'The average number of words in a summary is {avg_num_words}, max is {max_num_words}')\n",
    "print(f'The average number of sentences in a summary is {avg_num_periods}, max is {max_num_periods}')\n",
    "print(avg_num_words/avg_num_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"plots\"].str.count(\"\\n\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.86194193920325\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "#Alternative way to calculate plot length\n",
    "print(train_data[\"plots\"].str.lower().str.split().str.len().mean()) #Avg words\n",
    "print(train_data[\"plots\"].str.lower().str.split().str.len().max()) #Max words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567468\n",
      "16346270\n"
     ]
    }
   ],
   "source": [
    "dict_words = Counter(\" \".join(train_data[\"plots\"]).split(\" \"))\n",
    "print(len(dict_words.keys())) #should be number of unique words\n",
    "print(sum(dict_words.values())) #should be total number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/akhil.patel1896/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/akhil.patel1896/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Lemmetizing Function\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize w/lemmetization AFTER removing stopwords \n",
    "#https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "def tokenize(plot, stop_words, lemmatize = False):\n",
    "    \n",
    "    def re_sub(pattern, replace):\n",
    "        return re.sub(pattern, replace, plot)\n",
    "    \n",
    "    plot = plot.lower() #lowercase\n",
    "    plot = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,/.\\d]*\", \"DG\") #generic tag for numbers\n",
    "    plot = re_sub(r\"([!?.]){2,}\", r\"\\1\") #Convert multiple punctuations to the last punctuation mark\n",
    "    plot = plot.replace('-',' ') #separating hyphenated words\n",
    "    plot = plot.replace('_','') #remove underscores\n",
    "    plot = re_sub(r'(?<!\\w)([a-zA-Z])\\.', r'\\1') #remove periods from abbreviations\n",
    "    plot = re_sub('[^\\w\\s\\.\\?\\!\\']','') #remove punctuation besides sentence completers and apostrophes\n",
    "    sentences = nltk.sent_tokenize(plot)\n",
    "    words = list(map(nltk.word_tokenize, sentences))\n",
    "    words = [[x for x in w if not x in stop_words] for w in words]\n",
    "\n",
    "    if lemmatize:\n",
    "        output_lem = [nltk.pos_tag(w) for w in words]\n",
    "        return [[lemmatizer.lemmatize(x[0], pos = nltk2wn_tag(x[1])) for x in w] for w in output_lem]\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/akhil.patel1896/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/akhil.patel1896/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time to tokenize plots: 1484.1539039611816 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "full_data['tokenized_words'] = full_data.apply(lambda row: tokenize(row['plots'], stop, lemmatize = True), axis=1)\n",
    "end = time.time()\n",
    "print(\"Total Time to tokenize plots:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['flattened_tokens'] = full_data.apply(lambda l: [item for sublist in l['tokenized_words'] for item in sublist], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(full_data[\"list_genres_consol\"])\n",
    "full_data[\"binarized_labels\"] = labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Action', 'Adventure', 'Comedy', 'Crime', 'Drama', 'Family',\n",
       "       'Horror', 'Romance', 'Sci-Fi', 'Thriller'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224303, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_pickle(\"./full_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_pickle(\"./full_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>genres</th>\n",
       "      <th>plots</th>\n",
       "      <th>list_genres</th>\n",
       "      <th>list_genres_consol</th>\n",
       "      <th>USE_tokens</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>USE_tokens_sentences</th>\n",
       "      <th>flattened_tokens</th>\n",
       "      <th>binarized_labels</th>\n",
       "      <th>USE_token_plot_embeddings</th>\n",
       "      <th>USE_tokens_sentences_padded</th>\n",
       "      <th>USE_token_sentence_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>A stranded theatrical troupe manages to get ba...</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>a stranded theatrical troupe manages to get ba...</td>\n",
       "      <td>[[stranded, theatrical, troupe, manages, get, ...</td>\n",
       "      <td>[a stranded theatrical troupe manages to get b...</td>\n",
       "      <td>[stranded, theatrical, troupe, manages, get, b...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.06496927887201309, 0.02937605045735836, 0.0...</td>\n",
       "      <td>[a stranded theatrical troupe manages to get b...</td>\n",
       "      <td>[[0.04774859547615051, 0.03576108068227768, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "      <td>While waiting at the bus stop for the woman he...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>while waiting at the bus stop for the woman he...</td>\n",
       "      <td>[[wait, bus, stop, woman, intend, marry, jay, ...</td>\n",
       "      <td>[while waiting at the bus stop for the woman h...</td>\n",
       "      <td>[wait, bus, stop, woman, intend, marry, jay, d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.03805602714419365, 0.05806170403957367, -0....</td>\n",
       "      <td>[while waiting at the bus stop for the woman h...</td>\n",
       "      <td>[[0.030491432175040245, 0.00019499639165587723...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Policeman Lasse rehabilitates young prisoners ...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>policeman lasse rehabilitates young prisoners ...</td>\n",
       "      <td>[[policeman, lasse, rehabilitate, young, priso...</td>\n",
       "      <td>[policeman lasse rehabilitates young prisoners...</td>\n",
       "      <td>[policeman, lasse, rehabilitate, young, prison...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.031366996467113495, 0.0004756059206556529,...</td>\n",
       "      <td>[policeman lasse rehabilitates young prisoners...</td>\n",
       "      <td>[[-0.02940049022436142, -0.06734874099493027, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Adventure Animation Comedy</td>\n",
       "      <td>Patricia and Isobelle O'Sullivan arrives at th...</td>\n",
       "      <td>[Adventure, Animation, Comedy]</td>\n",
       "      <td>[Adventure, Comedy]</td>\n",
       "      <td>patricia and isobelle o'sullivan arrives at th...</td>\n",
       "      <td>[[patricia, isobelle, o'sullivan, arrive, st.,...</td>\n",
       "      <td>[patricia and isobelle o'sullivan arrives at t...</td>\n",
       "      <td>[patricia, isobelle, o'sullivan, arrive, st., ...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.06273579597473145, 0.042697515338659286, -...</td>\n",
       "      <td>[patricia and isobelle o'sullivan arrives at t...</td>\n",
       "      <td>[[0.014844884164631367, 0.0431971549987793, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Anna Zaccheo is a beautiful young woman from a...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>anna zaccheo is a beautiful young woman from a...</td>\n",
       "      <td>[[anna, zaccheo, beautiful, young, woman, work...</td>\n",
       "      <td>[anna zaccheo is a beautiful young woman from ...</td>\n",
       "      <td>[anna, zaccheo, beautiful, young, woman, work,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.03793998435139656, 0.03065059520304203, -0...</td>\n",
       "      <td>[anna zaccheo is a beautiful young woman from ...</td>\n",
       "      <td>[[-0.003618143266066909, 0.0034949216060340405...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                       genres  \\\n",
       "0      0                      Comedy    \n",
       "1      1                       Drama    \n",
       "2      3                       Drama    \n",
       "3      4  Adventure Animation Comedy    \n",
       "4      5                       Drama    \n",
       "\n",
       "                                               plots  \\\n",
       "0  A stranded theatrical troupe manages to get ba...   \n",
       "1  While waiting at the bus stop for the woman he...   \n",
       "2  Policeman Lasse rehabilitates young prisoners ...   \n",
       "3  Patricia and Isobelle O'Sullivan arrives at th...   \n",
       "4  Anna Zaccheo is a beautiful young woman from a...   \n",
       "\n",
       "                      list_genres   list_genres_consol  \\\n",
       "0                        [Comedy]             [Comedy]   \n",
       "1                         [Drama]              [Drama]   \n",
       "2                         [Drama]              [Drama]   \n",
       "3  [Adventure, Animation, Comedy]  [Adventure, Comedy]   \n",
       "4                         [Drama]              [Drama]   \n",
       "\n",
       "                                          USE_tokens  \\\n",
       "0  a stranded theatrical troupe manages to get ba...   \n",
       "1  while waiting at the bus stop for the woman he...   \n",
       "2  policeman lasse rehabilitates young prisoners ...   \n",
       "3  patricia and isobelle o'sullivan arrives at th...   \n",
       "4  anna zaccheo is a beautiful young woman from a...   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [[stranded, theatrical, troupe, manages, get, ...   \n",
       "1  [[wait, bus, stop, woman, intend, marry, jay, ...   \n",
       "2  [[policeman, lasse, rehabilitate, young, priso...   \n",
       "3  [[patricia, isobelle, o'sullivan, arrive, st.,...   \n",
       "4  [[anna, zaccheo, beautiful, young, woman, work...   \n",
       "\n",
       "                                USE_tokens_sentences  \\\n",
       "0  [a stranded theatrical troupe manages to get b...   \n",
       "1  [while waiting at the bus stop for the woman h...   \n",
       "2  [policeman lasse rehabilitates young prisoners...   \n",
       "3  [patricia and isobelle o'sullivan arrives at t...   \n",
       "4  [anna zaccheo is a beautiful young woman from ...   \n",
       "\n",
       "                                    flattened_tokens  \\\n",
       "0  [stranded, theatrical, troupe, manages, get, b...   \n",
       "1  [wait, bus, stop, woman, intend, marry, jay, d...   \n",
       "2  [policeman, lasse, rehabilitate, young, prison...   \n",
       "3  [patricia, isobelle, o'sullivan, arrive, st., ...   \n",
       "4  [anna, zaccheo, beautiful, young, woman, work,...   \n",
       "\n",
       "                                    binarized_labels  \\\n",
       "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                           USE_token_plot_embeddings  \\\n",
       "0  [0.06496927887201309, 0.02937605045735836, 0.0...   \n",
       "1  [0.03805602714419365, 0.05806170403957367, -0....   \n",
       "2  [-0.031366996467113495, 0.0004756059206556529,...   \n",
       "3  [-0.06273579597473145, 0.042697515338659286, -...   \n",
       "4  [-0.03793998435139656, 0.03065059520304203, -0...   \n",
       "\n",
       "                         USE_tokens_sentences_padded  \\\n",
       "0  [a stranded theatrical troupe manages to get b...   \n",
       "1  [while waiting at the bus stop for the woman h...   \n",
       "2  [policeman lasse rehabilitates young prisoners...   \n",
       "3  [patricia and isobelle o'sullivan arrives at t...   \n",
       "4  [anna zaccheo is a beautiful young woman from ...   \n",
       "\n",
       "                       USE_token_sentence_embeddings  \n",
       "0  [[0.04774859547615051, 0.03576108068227768, -0...  \n",
       "1  [[0.030491432175040245, 0.00019499639165587723...  \n",
       "2  [[-0.02940049022436142, -0.06734874099493027, ...  \n",
       "3  [[0.014844884164631367, 0.0431971549987793, 0....  \n",
       "4  [[-0.003618143266066909, 0.0034949216060340405...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over tokenized words and create dictionaries that keep track of number of tokens, length of sentences, and sentences per plot summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224303\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "sent_per_summary_dict = {}\n",
    "word_per_sent_dict = {}\n",
    "rows = len(full_data['tokenized_words'])\n",
    "print(rows)#number of plot summaries\n",
    "for i in range(len(full_data['tokenized_words'])):\n",
    "    length = len(full_data['tokenized_words'][i])\n",
    "    if length in sent_per_summary_dict:\n",
    "        sent_per_summary_dict[length] += 1\n",
    "    else:\n",
    "        sent_per_summary_dict[length] = 1\n",
    "    for j in range(length):\n",
    "        word_count = len(full_data['tokenized_words'][i][j])\n",
    "        if word_count in word_per_sent_dict:\n",
    "            word_per_sent_dict[word_count] += 1\n",
    "        else:\n",
    "            word_per_sent_dict[word_count] = 1\n",
    "        for word in full_data['tokenized_words'][i][j]:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221821\n",
      "11000055\n"
     ]
    }
   ],
   "source": [
    "print(len(word_dict.keys())) #should be number of unique words\n",
    "print(sum(word_dict.values())) #should be total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114657\n",
      "80476\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "twoOrOne = 0\n",
    "for value in word_dict.values():\n",
    "    if value == 1:\n",
    "        count +=1\n",
    "    if value <3:\n",
    "        twoOrOne +=1\n",
    "print(len(word_dict.keys()) - count) # words that appear more than once\n",
    "print(len(word_dict.keys()) - twoOrOne) # words that appear more than twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "1016033\n",
      "148\n",
      "12.386278792125847\n"
     ]
    }
   ],
   "source": [
    "print(len(word_per_sent_dict.keys())) #should be number of unique sentence lengths\n",
    "print(sum(word_per_sent_dict.values())) #should be number of sentences in all plots\n",
    "print(max(word_per_sent_dict.keys())) #should be largest sentence length\n",
    "total = 0\n",
    "weight_sum = 0\n",
    "for key, value in word_per_sent_dict.items():\n",
    "    total += value\n",
    "    weight_sum += key*value\n",
    "print(weight_sum/total) #should be average sentence length\n",
    "#print(word_per_sent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "28\n",
      "3.971159220333551\n"
     ]
    }
   ],
   "source": [
    "print(len(sent_per_summary_dict.keys())) #should be number of unique sentence lengths per summary\n",
    "print(max(sent_per_summary_dict.keys())) #should be highest amount of sentences per summary\n",
    "total = 0\n",
    "weight_sum = 0\n",
    "for key, value in sent_per_summary_dict.items():\n",
    "    total += value\n",
    "    weight_sum += key*value\n",
    "print(weight_sum/total) #should be average sentence count per summary\n",
    "#print(sent_per_summary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings\n",
    "embeddings_index = {}\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create average word vector. This will later be used in place of unknown words\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'), 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        pass\n",
    "n_vec = i + 1\n",
    "hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'), 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "average_vec = np.mean(vecs, axis=0)\n",
    "#print(average_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above token analysis, we set the following hyperparameters at these initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 15 #Gets ~80% of sentences\n",
    "MAX_SENTS = 5 #Gets ~80% of summaries\n",
    "MAX_NB_WORDS = 80000 #Eliminates words seen two or fewer times\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(full_data['flattened_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(full_data), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "doc_lst = []\n",
    "\n",
    "# keep the MAX_NB_WORDS most frequent words and replace the rest with 'UNK'\n",
    "# truncate to the first MAX_SENTS sentences per doc and MAX_SENT_LENGTH words per sentence\n",
    "\n",
    "for summary_num, row in full_data.iterrows():\n",
    "    for sent_num, sent in enumerate(row['tokenized_words']):\n",
    "        if sent_num < MAX_SENTS:\n",
    "            word_num = 0\n",
    "            words_in_sent = []\n",
    "            for _, word in enumerate(sent):\n",
    "                if word_num < MAX_SENT_LENGTH: \n",
    "                    try:\n",
    "                        if (word in tokenizer.word_index) and (tokenizer.word_index[word] < MAX_NB_WORDS):\n",
    "                            data[summary_num, sent_num, word_num] = tokenizer.word_index[word]\n",
    "                            words_in_sent.append(word)\n",
    "                        else:\n",
    "                            data[summary_num, sent_num, word_num] = MAX_NB_WORDS\n",
    "                            words_in_sent.append('UNK')\n",
    "                        word_num = word_num + 1\n",
    "                    except IndexError:\n",
    "                        print(summary_num)\n",
    "                        print(sent_num)\n",
    "                        print(word_num)\n",
    "                        print(row)\n",
    "                        print(word)\n",
    "            doc_lst.append(words_in_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 221821 unique tokens.\n",
      "Shape of data tensor: (224303, 5, 15)\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65067\n",
      "1\n",
      "156753\n"
     ]
    }
   ],
   "source": [
    "#leverage our embedding_index dictionary and our word_index to compute our embedding matrix\n",
    "embedding_matrix = np.zeros((MAX_NB_WORDS+1, EMBEDDING_DIM))\n",
    "count = 0\n",
    "unknown =0\n",
    "added =0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "    if embedding_vector is not None and i < MAX_NB_WORDS:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        added+=1\n",
    "    elif i == MAX_NB_WORDS:\n",
    "        # index MAX_NB_WORDS in data corresponds to 'UNK'\n",
    "        embedding_matrix[i] = average_vec #use average vector for unknown\n",
    "        unknown+=1\n",
    "    else:\n",
    "        count +=1\n",
    "print(added) #of the MAX_NB_WORDS most frequent tokens in our corpus, this many have GloVe embeddings\n",
    "print(unknown)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total absent words are 112713 which is 50.81 % of total words\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "absent_words = 0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        absent_words += 1\n",
    "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)), '% of total words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load this embedding matrix into an Embedding layer.\n",
    "REG_PARAM = 1e-13\n",
    "l2_reg = regularizers.l2(REG_PARAM)\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            embeddings_regularizer=l2_reg,\n",
    "                            mask_zero = True, #determines whether masking is performed, i.e. whether the layers ignore the padded zeros in shorter documents\n",
    "                            trainable=False) #prevent weights from being updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Hsankesara/DeepResearch/blob/master/Hierarchical_Attention_Network/attention_with_context.py\n",
    "#https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number  to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words level attention model\n",
    "word_input = Input(shape=(MAX_SENT_LENGTH,), dtype='float32')\n",
    "word_sequences = embedding_layer(word_input)\n",
    "word_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)\n",
    "word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)\n",
    "word_att = AttentionWithContext()(word_dense)\n",
    "wordEncoder = Model(word_input, word_att)\n",
    "\n",
    "# Sentence level attention model\n",
    "sent_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='float32')\n",
    "sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n",
    "sent_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)\n",
    "sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)\n",
    "sent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))\n",
    "preds = Dense(10, activation='sigmoid')(sent_att)\n",
    "model = Model(sent_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 5, 15)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 200)            67188400  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 5, 300)            421200    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 5, 200)            60200     \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 200)               40400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 67,712,210\n",
      "Trainable params: 1,165,610\n",
      "Non-trainable params: 66,546,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate training set into train and dev. Roughly 80% of original data is train, 10% dev, 10% test\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'han_food'\n",
    "history = History()\n",
    "csv_logger = CSVLogger('./{0}_{1}.log'.format(fname, REG_PARAM), separator=',', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88272, 5, 15)\n",
      "(88272, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.stack(new_train[\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.stack(new_train[\"binarized_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 216s - loss: 0.3144 - acc: 0.8625\n",
      "Epoch 2/10\n",
      " - 217s - loss: 0.2873 - acc: 0.8759\n",
      "Epoch 3/10\n",
      " - 216s - loss: 0.2641 - acc: 0.8871\n",
      "Epoch 4/10\n",
      " - 218s - loss: 0.2433 - acc: 0.8971\n",
      "Epoch 5/10\n",
      " - 216s - loss: 0.2249 - acc: 0.9060\n",
      "Epoch 6/10\n",
      " - 216s - loss: 0.2088 - acc: 0.9135\n",
      "Epoch 7/10\n",
      " - 217s - loss: 0.1929 - acc: 0.9207\n",
      "Epoch 8/10\n",
      " - 216s - loss: 0.1778 - acc: 0.9279\n",
      "Epoch 9/10\n",
      " - 216s - loss: 0.1610 - acc: 0.9355\n",
      "Epoch 10/10\n",
      " - 217s - loss: 0.1485 - acc: 0.9412\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=False, \n",
    "          callbacks=[history, csv_logger], verbose=2)\n",
    "\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114730, 5, 15)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = np.stack(dev_set[\"embedding\"])\n",
    "y_dev = np.stack(dev_set[\"binarized_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22206, 5, 15)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.stack(test_set[\"embedding\"])\n",
    "y_test = np.stack(test_set[\"binarized_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections implement and evaluate models trained on the entire data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indiv_class_scores(y_true, y_pred, threshold, metric = None):\n",
    "    max_pred = np.array(y_pred.max(axis = 1)).reshape(-1,1)\n",
    "    \n",
    "    #Cant predict 0 for all classes - pick max val as label if all preds below threshold\n",
    "    for i in range(len(max_pred)): \n",
    "        if max_pred[i] > threshold:\n",
    "            max_pred[i] = threshold\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    y_pred = y_pred >= max_pred\n",
    "    \n",
    "    for i in range(len(mlb.classes_)):\n",
    "        if metric == \"precision\":\n",
    "            score = precision_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        elif metric == \"recall\":\n",
    "            score = recall_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        elif metric == \"f1\":\n",
    "            score = f1_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        else:\n",
    "            return \"Not a valid metric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_pr(y_true, y_pred):\n",
    "    scores = []\n",
    "    for i in range(len(mlb.classes_)):\n",
    "        p, r, d = precision_recall_curve(y_true[:,i], y_pred[:,i])\n",
    "        score = auc(r,p)\n",
    "        scores.append(score)\n",
    "        print(\"The AUC for {} is {}\".format(mlb.classes_[i], score))\n",
    "    macro_score = np.mean(scores)\n",
    "    y_true_raveled = np.ravel(y_true)\n",
    "    y_pred_raveled = np.ravel(y_pred)\n",
    "    p, r, d = precision_recall_curve(y_true_raveled, y_pred_raveled)\n",
    "    micro_score = auc(r,p)\n",
    "    print(\"The micro-avg AUC is {}\".format(micro_score))\n",
    "    print(\"The macro-avg AUC is {}\".format(macro_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indiv_class_scores_ensemble(y_true, y_pred, threshold):\n",
    "    \"\"\"y_pred is a dictionary of predictions while\n",
    "        y_true is an array of predictions for the dev or test set\"\"\"\n",
    "    \n",
    "    list_of_predictions = []\n",
    "    \n",
    "    for preds in list(y_pred.values()):\n",
    "        max_pred = np.array(preds.max(axis = 1)).reshape(-1,1)\n",
    "    \n",
    "        #Cant predict 0 for all classes - pick max val as label if all preds below threshold\n",
    "        for i in range(len(max_pred)): \n",
    "            if max_pred[i] > threshold:\n",
    "                max_pred[i] = threshold\n",
    "            else:\n",
    "                pass\n",
    "        list_of_predictions.append(preds >= max_pred)\n",
    "    \n",
    "    voted_preds = sum(list_of_predictions)\n",
    "    \n",
    "    max_voted_preds = np.array(voted_preds.max(axis = 1)).reshape(-1,1)\n",
    "    \n",
    "    for i in range(len(max_voted_preds)): \n",
    "        if max_voted_preds[i] > 2.5:\n",
    "            max_voted_preds[i] = 2.5\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    y_pred = np.where(voted_preds >= max_voted_preds, 1, 0) #3 for yes is needed to predict a class\n",
    "    \n",
    "    for i in range(len(mlb.classes_)):\n",
    "        score = precision_score(y_true[:,i], y_pred[:,i])\n",
    "        print(\"The precision for {} is {}\".format(mlb.classes_[i], score))\n",
    "    \n",
    "    for i in range(len(mlb.classes_)):  \n",
    "        score = recall_score(y_true[:,i], y_pred[:,i])\n",
    "        print(\"The recall for {} is {}\".format(mlb.classes_[i], score))\n",
    "        \n",
    "    for i in range(len(mlb.classes_)): \n",
    "        score = f1_score(y_true[:,i], y_pred[:,i])\n",
    "        print(\"The f1 for {} is {}\".format(mlb.classes_[i], score))\n",
    "    \n",
    "    micro_precision = precision_score(y_true, y_pred, average = 'micro')\n",
    "    weighted_macro_precision = precision_score(y_true, y_pred, average = 'weighted')\n",
    "    micro_recall = recall_score(y_true, y_pred, average = 'micro')\n",
    "    weighted_macro_recall = recall_score(y_true, y_pred, average = 'weighted')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average = 'micro')\n",
    "    weighted_macro_f1 = f1_score(y_true, y_pred, average = 'weighted')\n",
    "    print(\"The micro precision is\", micro_precision)\n",
    "    print(\"The weighted macro precision is\", weighted_macro_precision)\n",
    "    print(\"The micro recall is\", micro_recall)\n",
    "    print(\"The weighted macro recall is\", weighted_macro_recall)\n",
    "    print(\"The micro f1 is\", micro_f1)\n",
    "    print(\"The weighted macro f1 is\", weighted_macro_f1)\n",
    "    \n",
    "    num_preds = y_pred.sum(axis = 1)\n",
    "    avg_genre_per_pred = np.average(num_preds)\n",
    "    median_genre_per_pred = np.median(num_preds)\n",
    "    print(\"The average number of genres per movie is\", avg_genre_per_pred)\n",
    "    print(\"The median number of genres per movie is\", median_genre_per_pred)\n",
    "        \n",
    "    #return list_of_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_pr_ensemble(y_true, y_pred):\n",
    "    y_pred = np.mean(list(y_pred.values()), axis = 0)\n",
    "    scores = []\n",
    "    for i in range(len(mlb.classes_)):\n",
    "        p, r, d = precision_recall_curve(y_true[:,i], y_pred[:,i])\n",
    "        score = auc(r,p)\n",
    "        scores.append(score)\n",
    "        print(\"The AUC for {} is {}\".format(mlb.classes_[i], score))\n",
    "    macro_score = np.mean(scores)\n",
    "    y_true_raveled = np.ravel(y_true)\n",
    "    y_pred_raveled = np.ravel(y_pred)\n",
    "    p, r, d = precision_recall_curve(y_true_raveled, y_pred_raveled)\n",
    "    micro_score = auc(r,p)\n",
    "    print(\"The micro-avg AUC is {}\".format(micro_score))\n",
    "    print(\"The macro-avg AUC is {}\".format(macro_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_pickle(\"./local/full_data.pkl\")\n",
    "USE_sent_embed = np.load(\"./local/USE_token_sentence_embeddings.npy\")\n",
    "full_data[\"USE_token_sentence_embeddings\"] = USE_sent_embed.tolist()\n",
    "full_data = full_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['embedding'] = data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(full_data, test_size=0.10, random_state=42)\n",
    "\n",
    "#Separate training set into train and dev. Roughly 80% of original data is train, 10% dev, 10% test\n",
    "train_set, dev_set = train_test_split(train_set, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_w_out_replacement(df, genre_counts):\n",
    "    \"\"\"There can be duplicates across genre samples, but not within genre samples\"\"\"\n",
    "    max_records = sorted(genre_counts.items(), key=operator.itemgetter(1))[0][1] #num records for min class\n",
    "    new_df = pd.DataFrame(columns=list(df.columns))\n",
    "    for i in list(genre_counts.keys()):\n",
    "        print(i)\n",
    "        sample_df = df[df[\"list_genres_consol\"].apply(lambda x: str(i) in x)]\n",
    "        new_df = new_df.append(sample_df.sample(max_records, replace = False))\n",
    "    return new_df.sample(frac=1) #shuffles the sample dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sci-Fi', 11473),\n",
       " ('Adventure', 14310),\n",
       " ('Crime', 14942),\n",
       " ('Horror', 15978),\n",
       " ('Family', 16595),\n",
       " ('Action', 19939),\n",
       " ('Romance', 22628),\n",
       " ('Thriller', 22904),\n",
       " ('Comedy', 57193),\n",
       " ('Drama', 93987)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "#Distribution of Genres - imbalanced \n",
    "count_dict_norm = defaultdict(int)\n",
    "\n",
    "def dict_count(row, count_dict):\n",
    "    for genre in row:\n",
    "        count_dict[genre] += 1\n",
    "\n",
    "        \n",
    "count_dict_series = train_set[\"list_genres_consol\"].apply(lambda row: dict_count(row, count_dict_norm))\n",
    "\n",
    "sorted(count_dict_norm.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drama\n",
      "Action\n",
      "Romance\n",
      "Horror\n",
      "Comedy\n",
      "Sci-Fi\n",
      "Family\n",
      "Adventure\n",
      "Thriller\n",
      "Crime\n",
      "total time:  4.4035234451293945 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "new_train = undersample_w_out_replacement(train_set, count_dict_norm)\n",
    "end = time.time()\n",
    "print(\"total time: \", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drama\n",
      "Action\n",
      "Romance\n",
      "Horror\n",
      "Comedy\n",
      "Sci-Fi\n",
      "Family\n",
      "Adventure\n",
      "Thriller\n",
      "Crime\n",
      "Drama\n",
      "Action\n",
      "Romance\n",
      "Horror\n",
      "Comedy\n",
      "Sci-Fi\n",
      "Family\n",
      "Adventure\n",
      "Thriller\n",
      "Crime\n",
      "Drama\n",
      "Action\n",
      "Romance\n",
      "Horror\n",
      "Comedy\n",
      "Sci-Fi\n",
      "Family\n",
      "Adventure\n",
      "Thriller\n",
      "Crime\n",
      "Drama\n",
      "Action\n",
      "Romance\n",
      "Horror\n",
      "Comedy\n",
      "Sci-Fi\n",
      "Family\n",
      "Adventure\n",
      "Thriller\n",
      "Crime\n",
      "Drama\n",
      "Action\n",
      "Romance\n",
      "Horror\n",
      "Comedy\n",
      "Sci-Fi\n",
      "Family\n",
      "Adventure\n",
      "Thriller\n",
      "Crime\n"
     ]
    }
   ],
   "source": [
    "sample_dfs = {}\n",
    "for i in range(1,6):\n",
    "    sample_dfs[\"model{}\".format(i)] = undersample_w_out_replacement(train_set, count_dict_norm)\n",
    "    \n",
    "count_dict_nested = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "def nested_dict_count(row, count_dict, model):\n",
    "    for genre in row:\n",
    "        count_dict_nested[model][genre] += 1\n",
    "\n",
    "for i in range(1,6):\n",
    "    count_val_series_2 = sample_dfs[\"model{}\".format(i)][\"list_genres_consol\"].apply(lambda row: nested_dict_count(row, count_dict_nested, \n",
    "                                                                                     \"model{}\".format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_HAN_model(sample, file, df):\n",
    "    # Words level attention model\n",
    "    word_input = Input(shape=(MAX_SENT_LENGTH,), dtype='float32')\n",
    "    word_sequences = embedding_layer(word_input)\n",
    "    word_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)\n",
    "    word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)\n",
    "    word_att = AttentionWithContext()(word_dense)\n",
    "    wordEncoder = Model(word_input, word_att)\n",
    "\n",
    "    # Sentence level attention model\n",
    "    sent_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='float32')\n",
    "    sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n",
    "    sent_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)\n",
    "    sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)\n",
    "    sent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))\n",
    "    preds = Dense(10, activation='sigmoid')(sent_att)\n",
    "    model = Model(sent_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [\"accuracy\"])\n",
    "    model.fit(sample, np.stack(df[\"binarized_labels\"]), epochs = 10, batch_size = 500)\n",
    "    print(\"Predicting\")\n",
    "    predict = model.predict(x_dev)\n",
    "    model.save(file)\n",
    "    return predict\n",
    "\n",
    "def train_HAN_samples(df_dict):\n",
    "\n",
    "    model_num = 1\n",
    "    preds_group = {}\n",
    "\n",
    "    for i in list(df_dict.values()):\n",
    "        print(\"Training model \" + str(model_num))\n",
    "        x_train = np.stack(i[\"embedding\"])\n",
    "        preds_group[\"preds_{}\".format(model_num)] = train_HAN_model(x_train,\n",
    "                                                               \"./local/HAN_300dim_500batch_dropout_sample_{}.h5\".format(model_num),\n",
    "                                                                         i)\n",
    "        model_num += 1\n",
    "        \n",
    "    return preds_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 223s 2ms/step - loss: 0.4272 - acc: 0.8068\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 210s 2ms/step - loss: 0.3807 - acc: 0.8283\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 210s 2ms/step - loss: 0.3643 - acc: 0.8364\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.3481 - acc: 0.8444\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.3304 - acc: 0.8536\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 215s 2ms/step - loss: 0.3104 - acc: 0.8638\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 213s 2ms/step - loss: 0.2893 - acc: 0.8742\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 211s 2ms/step - loss: 0.2687 - acc: 0.8846\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 209s 2ms/step - loss: 0.2479 - acc: 0.8949\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 213s 2ms/step - loss: 0.2292 - acc: 0.9037\n",
      "Predicting\n",
      "Training model 2\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 224s 2ms/step - loss: 0.4271 - acc: 0.8069\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 216s 2ms/step - loss: 0.3820 - acc: 0.8274\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 213s 2ms/step - loss: 0.3659 - acc: 0.8352\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 213s 2ms/step - loss: 0.3497 - acc: 0.8431\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 213s 2ms/step - loss: 0.3319 - acc: 0.8519\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 213s 2ms/step - loss: 0.3124 - acc: 0.8619\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 210s 2ms/step - loss: 0.2920 - acc: 0.8724\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.2707 - acc: 0.8831\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 211s 2ms/step - loss: 0.2502 - acc: 0.8933\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.2311 - acc: 0.9029\n",
      "Predicting\n",
      "Training model 3\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 223s 2ms/step - loss: 0.4278 - acc: 0.8065\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 211s 2ms/step - loss: 0.3817 - acc: 0.8277\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 208s 2ms/step - loss: 0.3657 - acc: 0.8354\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.3502 - acc: 0.8432\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.3324 - acc: 0.8521\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 210s 2ms/step - loss: 0.3132 - acc: 0.8622\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 209s 2ms/step - loss: 0.2933 - acc: 0.8719\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 210s 2ms/step - loss: 0.2725 - acc: 0.8826\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 213s 2ms/step - loss: 0.2521 - acc: 0.8923\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.2324 - acc: 0.9018\n",
      "Predicting\n",
      "Training model 4\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 225s 2ms/step - loss: 0.4288 - acc: 0.8063\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 211s 2ms/step - loss: 0.3820 - acc: 0.8276\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 212s 2ms/step - loss: 0.3660 - acc: 0.8353\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 211s 2ms/step - loss: 0.3495 - acc: 0.8434\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.3317 - acc: 0.8526\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 212s 2ms/step - loss: 0.3117 - acc: 0.8625\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 215s 2ms/step - loss: 0.2910 - acc: 0.8734\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 213s 2ms/step - loss: 0.2698 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 211s 2ms/step - loss: 0.2500 - acc: 0.8936\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 208s 2ms/step - loss: 0.2303 - acc: 0.9033\n",
      "Predicting\n",
      "Training model 5\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 220s 2ms/step - loss: 0.4276 - acc: 0.8071\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 211s 2ms/step - loss: 0.3814 - acc: 0.8278\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 213s 2ms/step - loss: 0.3652 - acc: 0.8358\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 210s 2ms/step - loss: 0.3489 - acc: 0.8439\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.3311 - acc: 0.8528\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 211s 2ms/step - loss: 0.3118 - acc: 0.8630\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 212s 2ms/step - loss: 0.2907 - acc: 0.8737\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 212s 2ms/step - loss: 0.2705 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 214s 2ms/step - loss: 0.2496 - acc: 0.8939\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 211s 2ms/step - loss: 0.2293 - acc: 0.9039\n",
      "Predicting\n"
     ]
    }
   ],
   "source": [
    "preds_dict = train_HAN_samples(sample_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for Action is 0.4438549955791335\n",
      "The precision for Adventure is 0.3723916532905297\n",
      "The precision for Comedy is 0.6248620816476645\n",
      "The precision for Crime is 0.3698461538461538\n",
      "The precision for Drama is 0.6823243860651057\n",
      "The precision for Family is 0.34130320546505516\n",
      "The precision for Horror is 0.48442422121106055\n",
      "The precision for Romance is 0.3291193486179559\n",
      "The precision for Sci-Fi is 0.48560273304050755\n",
      "The precision for Thriller is 0.3728498372849837\n",
      "The recall for Action is 0.6055488540410132\n",
      "The recall for Adventure is 0.5161290322580645\n",
      "The recall for Comedy is 0.7210355071438675\n",
      "The recall for Crime is 0.6448497854077253\n",
      "The recall for Drama is 0.8291117279666897\n",
      "The recall for Family is 0.6197519083969466\n",
      "The recall for Horror is 0.7011144883485309\n",
      "The recall for Romance is 0.5634629493763756\n",
      "The recall for Sci-Fi is 0.6718433490884538\n",
      "The recall for Thriller is 0.5681898689337584\n",
      "The f1 for Action is 0.5122448979591837\n",
      "The f1 for Adventure is 0.4326340326340326\n",
      "The f1 for Comedy is 0.669512675686326\n",
      "The f1 for Crime is 0.470082127493156\n",
      "The f1 for Drama is 0.7485902255639099\n",
      "The f1 for Family is 0.4401897661809556\n",
      "The f1 for Horror is 0.5729662595735873\n",
      "The f1 for Romance is 0.4155282023535777\n",
      "The f1 for Sci-Fi is 0.5637393767705382\n",
      "The f1 for Thriller is 0.4502456140350877\n",
      "The micro precision is 0.512627345297156\n",
      "The weighted macro precision is 0.5324539248190376\n",
      "The micro recall is 0.7004686715393629\n",
      "The weighted macro recall is 0.7004686715393629\n",
      "The micro f1 is 0.5920049040989307\n",
      "The weighted macro f1 is 0.602101762361846\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores_ensemble(y_dev, preds_dict, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_USE_HAN_model(sample, file, df):\n",
    "    # Sentence level attention model)\n",
    "    sent_input = Input(shape=(MAX_SENTS, 512), dtype='float32')\n",
    "    sent_sequences = Lambda(lambda x:x,output_shape=(MAX_SENTS, 512))(sent_input) #UniversalEmbedding,output_shape=(embed_size,))(input_text)\n",
    "    sent_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_sequences)\n",
    "    sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)\n",
    "    sent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))\n",
    "    preds = Dense(10, activation='sigmoid')(sent_att)\n",
    "    model = Model(sent_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [\"accuracy\"])\n",
    "    model.fit(sample, np.stack(df[\"binarized_labels\"]), epochs = 10, batch_size = 500)\n",
    "    print(\"Predicting\")\n",
    "    predict = model.predict(x_dev)\n",
    "    model.save(file)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_USE_HAN_samples(df_dict):\n",
    "\n",
    "    model_num = 1\n",
    "    preds_group = {}\n",
    "\n",
    "    for i in list(df_dict.values()):\n",
    "        print(\"Training model \" + str(model_num))\n",
    "        x_train = np.stack(i[\"USE_token_sentence_embeddings\"])\n",
    "        preds_group[\"preds_{}\".format(model_num)] = train_USE_HAN_model(x_train,\n",
    "                                                               \"./local/USE_HAN_300dim_500batch_dropout_sample_{}.h5\".format(model_num),\n",
    "                                                                         i)\n",
    "        model_num += 1\n",
    "        \n",
    "    return preds_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 33s 284us/step - loss: 0.4326 - acc: 0.8041\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 29s 256us/step - loss: 0.3895 - acc: 0.8226\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 29s 252us/step - loss: 0.3828 - acc: 0.8262\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 29s 253us/step - loss: 0.3787 - acc: 0.8282\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 29s 252us/step - loss: 0.3752 - acc: 0.8298\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 29s 254us/step - loss: 0.3722 - acc: 0.8316\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 29s 254us/step - loss: 0.3694 - acc: 0.8327\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 29s 252us/step - loss: 0.3666 - acc: 0.8341\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 29s 253us/step - loss: 0.3639 - acc: 0.8356\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 30s 262us/step - loss: 0.3609 - acc: 0.8370\n",
      "Predicting\n",
      "Training model 2\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 33s 292us/step - loss: 0.4341 - acc: 0.8035\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 30s 263us/step - loss: 0.3903 - acc: 0.8225\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 29s 254us/step - loss: 0.3834 - acc: 0.8260\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 29s 253us/step - loss: 0.3790 - acc: 0.8283\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 30s 264us/step - loss: 0.3754 - acc: 0.8301\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 31s 271us/step - loss: 0.3726 - acc: 0.8315\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 29s 253us/step - loss: 0.3694 - acc: 0.8330\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 29s 253us/step - loss: 0.3668 - acc: 0.8343\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 29s 254us/step - loss: 0.3634 - acc: 0.8358\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 30s 261us/step - loss: 0.3608 - acc: 0.8372\n",
      "Predicting\n",
      "Training model 3\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 34s 292us/step - loss: 0.4362 - acc: 0.8028\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 29s 253us/step - loss: 0.3902 - acc: 0.8226\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 29s 251us/step - loss: 0.3831 - acc: 0.8262\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 29s 253us/step - loss: 0.3785 - acc: 0.8286\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 29s 254us/step - loss: 0.3748 - acc: 0.8304\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 29s 253us/step - loss: 0.3711 - acc: 0.8322\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 29s 257us/step - loss: 0.3683 - acc: 0.8336\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 29s 255us/step - loss: 0.3654 - acc: 0.8352\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 30s 259us/step - loss: 0.3621 - acc: 0.8365\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 30s 261us/step - loss: 0.3591 - acc: 0.8384\n",
      "Predicting\n",
      "Training model 4\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 34s 298us/step - loss: 0.4369 - acc: 0.8026\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 29s 255us/step - loss: 0.3899 - acc: 0.8227\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 29s 255us/step - loss: 0.3833 - acc: 0.8260\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 29s 255us/step - loss: 0.3790 - acc: 0.8285\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 29s 254us/step - loss: 0.3754 - acc: 0.8303\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 29s 254us/step - loss: 0.3722 - acc: 0.8315\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 29s 254us/step - loss: 0.3693 - acc: 0.8332\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 29s 254us/step - loss: 0.3664 - acc: 0.8342\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 30s 264us/step - loss: 0.3637 - acc: 0.8358\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 29s 256us/step - loss: 0.3610 - acc: 0.8370\n",
      "Predicting\n",
      "Training model 5\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 35s 301us/step - loss: 0.4363 - acc: 0.8025\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 30s 260us/step - loss: 0.3905 - acc: 0.8225\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 30s 259us/step - loss: 0.3836 - acc: 0.8257\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 30s 260us/step - loss: 0.3790 - acc: 0.8283\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 30s 260us/step - loss: 0.3753 - acc: 0.8300\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 30s 261us/step - loss: 0.3721 - acc: 0.8319\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 30s 263us/step - loss: 0.3689 - acc: 0.8331\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 31s 266us/step - loss: 0.3657 - acc: 0.8345\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 30s 265us/step - loss: 0.3630 - acc: 0.8357\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 30s 265us/step - loss: 0.3594 - acc: 0.8379\n",
      "Predicting\n"
     ]
    }
   ],
   "source": [
    "USE_preds_dict = train_USE_HAN_samples(sample_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = np.stack(dev_set[\"USE_token_plot_embeddings\"])\n",
    "y_dev = np.stack(dev_set[\"binarized_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for Action is 0.5462559639924255\n",
      "The AUC for Adventure is 0.4300937378035205\n",
      "The AUC for Comedy is 0.7174280736870873\n",
      "The AUC for Crime is 0.4299795274999502\n",
      "The AUC for Drama is 0.7950425362264104\n",
      "The AUC for Family is 0.48230767353328247\n",
      "The AUC for Horror is 0.6086722586490639\n",
      "The AUC for Romance is 0.3841523903354247\n",
      "The AUC for Sci-Fi is 0.632598135728725\n",
      "The AUC for Thriller is 0.41421208365987094\n",
      "The micro-avg AUC is 0.6224622140237992\n",
      "The macro-avg AUC is 0.5440742381115761\n"
     ]
    }
   ],
   "source": [
    "auc_pr_ensemble(y_dev, USE_preds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for Action is 0.5166051660516605\n",
      "The precision for Adventure is 0.4343867166577397\n",
      "The precision for Comedy is 0.691073219658977\n",
      "The precision for Crime is 0.4192521877486078\n",
      "The precision for Drama is 0.7143422197610948\n",
      "The precision for Family is 0.4357588357588358\n",
      "The precision for Horror is 0.5417028670721112\n",
      "The precision for Romance is 0.3828102082722206\n",
      "The precision for Sci-Fi is 0.5800508259212198\n",
      "The precision for Thriller is 0.4208765473402476\n",
      "The recall for Action is 0.5629272215520708\n",
      "The recall for Adventure is 0.4510567296996663\n",
      "The recall for Comedy is 0.5848069033809591\n",
      "The recall for Crime is 0.5654506437768241\n",
      "The recall for Drama is 0.7833102012491325\n",
      "The recall for Family is 0.5\n",
      "The recall for Horror is 0.6317122593718338\n",
      "The recall for Romance is 0.4787234042553192\n",
      "The recall for Sci-Fi is 0.6164753544902093\n",
      "The recall for Thriller is 0.44562522139567834\n",
      "The f1 for Action is 0.5387723686742352\n",
      "The f1 for Adventure is 0.44256480218281036\n",
      "The f1 for Comedy is 0.6335146732051185\n",
      "The f1 for Crime is 0.4814984010963911\n",
      "The f1 for Drama is 0.7472381976912574\n",
      "The f1 for Family is 0.46567429460119975\n",
      "The f1 for Horror is 0.5832553788587466\n",
      "The f1 for Romance is 0.4254278728606357\n",
      "The f1 for Sci-Fi is 0.597708674304419\n",
      "The f1 for Thriller is 0.43289745354439085\n",
      "The micro precision is 0.5782304065845336\n",
      "The weighted macro precision is 0.5869789241502956\n",
      "The micro recall is 0.6193159627294538\n",
      "The weighted macro recall is 0.6193159627294538\n",
      "The micro f1 is 0.5980684007058285\n",
      "The weighted macro f1 is 0.600347052990655\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores_ensemble(y_dev, USE_preds_dict, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_USE_HAN_fulldata(data):\n",
    "    model_num = 1\n",
    "    preds_group = {}\n",
    "\n",
    "    print(\"Training model \" + str(model_num))\n",
    "    x_train = np.stack(data[\"USE_token_sentence_embeddings\"])\n",
    "    preds_group[\"preds_{}\".format(model_num)] = train_USE_HAN_model(x_train,\n",
    "                                                               \"./local/USE_HAN_300dim_500batch_dropout_sample_{}.h5\".format(model_num),\n",
    "                                                                         i)\n",
    "    model_num += 1\n",
    "        \n",
    "    return preds_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "179666/179666 [==============================] - 52s 290us/step - loss: 0.3278 - acc: 0.8662\n",
      "Epoch 2/10\n",
      "179666/179666 [==============================] - 47s 263us/step - loss: 0.2937 - acc: 0.8767\n",
      "Epoch 3/10\n",
      "179666/179666 [==============================] - 48s 265us/step - loss: 0.2890 - acc: 0.8787\n",
      "Epoch 4/10\n",
      "179666/179666 [==============================] - 47s 262us/step - loss: 0.2860 - acc: 0.8801\n",
      "Epoch 5/10\n",
      "179666/179666 [==============================] - 46s 259us/step - loss: 0.2835 - acc: 0.8811\n",
      "Epoch 6/10\n",
      "179666/179666 [==============================] - 48s 269us/step - loss: 0.2816 - acc: 0.8819\n",
      "Epoch 7/10\n",
      "179666/179666 [==============================] - 47s 260us/step - loss: 0.2794 - acc: 0.8827\n",
      "Epoch 8/10\n",
      "179666/179666 [==============================] - 47s 262us/step - loss: 0.2780 - acc: 0.8834\n",
      "Epoch 9/10\n",
      "179666/179666 [==============================] - 48s 270us/step - loss: 0.2764 - acc: 0.8840\n",
      "Epoch 10/10\n",
      "179666/179666 [==============================] - 48s 270us/step - loss: 0.2749 - acc: 0.8846\n",
      "Predicting\n"
     ]
    }
   ],
   "source": [
    "fulldata_pred = train_USE_HAN_model(np.stack(train_set[\"USE_token_sentence_embeddings\"]), \"./local/USE_HAN_300dim_500batch_dropout_fulldata.h5\",train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for Action is 0.5527689577437199\n",
      "The AUC for Adventure is 0.4422147363418738\n",
      "The AUC for Comedy is 0.7305358593913303\n",
      "The AUC for Crime is 0.44035213018055847\n",
      "The AUC for Drama is 0.8085340420157743\n",
      "The AUC for Family is 0.4966782864470115\n",
      "The AUC for Horror is 0.6113529062565832\n",
      "The AUC for Romance is 0.3995672757487425\n",
      "The AUC for Sci-Fi is 0.6287086303480689\n",
      "The AUC for Thriller is 0.4170264439103551\n",
      "The micro-avg AUC is 0.6661143960200353\n",
      "The macro-avg AUC is 0.5527739268384018\n"
     ]
    }
   ],
   "source": [
    "auc_pr(y_dev, fulldata_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fulldata_pred >= 0.5\n",
    "micro_precision = precision_score(y_dev, y_pred, average = 'micro')\n",
    "weighted_macro_precision = precision_score(y_dev, y_pred, average = 'weighted')\n",
    "micro_recall = recall_score(y_dev, y_pred, average = 'micro')\n",
    "weighted_macro_recall = recall_score(y_dev, y_pred, average = 'weighted')\n",
    "micro_f1 = f1_score(y_dev, y_pred, average = 'micro')\n",
    "weighted_macro_f1 = f1_score(y_dev, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The micro precision is 0.7024703317994672\n",
      "The weighted macro precision is 0.6807239201560332\n",
      "The micro recall is 0.4854934999721029\n",
      "The weighted macro recall is 0.4854934999721029\n",
      "The micro f1 is 0.5741669416034312\n",
      "The weighted macro f1 is 0.5378685849368693\n"
     ]
    }
   ],
   "source": [
    "print(\"The micro precision is\", micro_precision)\n",
    "print(\"The weighted macro precision is\", weighted_macro_precision)\n",
    "print(\"The micro recall is\", micro_recall)\n",
    "print(\"The weighted macro recall is\", weighted_macro_recall)\n",
    "print(\"The micro f1 is\", micro_f1)\n",
    "print(\"The weighted macro f1 is\", weighted_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_USE_FC_model(sample, file, df):\n",
    "    sent_input = Input(shape=(512,), dtype='float32')\n",
    "    sent_sequences = Lambda(lambda x:x,output_shape=(512,))(sent_input) \n",
    "    sent_dense = Dense(200, kernel_regularizer=l2_reg)(sent_sequences)\n",
    "    post_drop = Dropout(0.5)(sent_dense)\n",
    "    preds = Dense(10, activation='sigmoid')(post_drop)\n",
    "    model = Model(sent_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [\"accuracy\"])\n",
    "    model.fit(sample, np.stack(df[\"binarized_labels\"]), epochs = 10, batch_size = 500)\n",
    "    print(\"Predicting\")\n",
    "    predict = model.predict(x_dev)\n",
    "    model.save(file)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "179666/179666 [==============================] - 6s 36us/step - loss: 0.3288 - acc: 0.8661\n",
      "Epoch 2/10\n",
      "179666/179666 [==============================] - 4s 21us/step - loss: 0.2972 - acc: 0.8749\n",
      "Epoch 3/10\n",
      "179666/179666 [==============================] - 4s 21us/step - loss: 0.2950 - acc: 0.8760\n",
      "Epoch 4/10\n",
      "179666/179666 [==============================] - 4s 21us/step - loss: 0.2938 - acc: 0.8763\n",
      "Epoch 5/10\n",
      "179666/179666 [==============================] - 4s 21us/step - loss: 0.2931 - acc: 0.8768\n",
      "Epoch 6/10\n",
      "179666/179666 [==============================] - 4s 22us/step - loss: 0.2923 - acc: 0.8770\n",
      "Epoch 7/10\n",
      "179666/179666 [==============================] - 4s 22us/step - loss: 0.2920 - acc: 0.8772\n",
      "Epoch 8/10\n",
      "179666/179666 [==============================] - 4s 22us/step - loss: 0.2914 - acc: 0.8774\n",
      "Epoch 9/10\n",
      "179666/179666 [==============================] - 4s 22us/step - loss: 0.2909 - acc: 0.8777\n",
      "Epoch 10/10\n",
      "179666/179666 [==============================] - 4s 21us/step - loss: 0.2907 - acc: 0.8778\n",
      "Predicting\n"
     ]
    }
   ],
   "source": [
    "fulldata_USE_FC_pred = train_USE_FC_model(np.stack(train_set[\"USE_token_plot_embeddings\"]), \"./local/USE_FC_500batch_dropout_fulldata.h5\",train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for Action is 0.5453001761711098\n",
      "The AUC for Adventure is 0.3957226378873043\n",
      "The AUC for Comedy is 0.6974327702002612\n",
      "The AUC for Crime is 0.41839870432787946\n",
      "The AUC for Drama is 0.7841624367518782\n",
      "The AUC for Family is 0.45220466134752396\n",
      "The AUC for Horror is 0.5607895843008089\n",
      "The AUC for Romance is 0.38483609153557025\n",
      "The AUC for Sci-Fi is 0.5991009093403481\n",
      "The AUC for Thriller is 0.3869944846698701\n",
      "The micro-avg AUC is 0.6390774992517999\n",
      "The macro-avg AUC is 0.5224942456532553\n"
     ]
    }
   ],
   "source": [
    "auc_pr(y_dev, fulldata_USE_FC_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The micro precision is 0.6964494732735076\n",
      "The weighted macro precision is 0.6747163963318376\n",
      "The micro recall is 0.44816715951570607\n",
      "The weighted macro recall is 0.44816715951570607\n",
      "The micro f1 is 0.5453804762955545\n",
      "The weighted macro f1 is 0.49958371667356677\n"
     ]
    }
   ],
   "source": [
    "y_pred = fulldata_USE_FC_pred >= 0.5\n",
    "micro_precision = precision_score(y_dev, y_pred, average = 'micro')\n",
    "weighted_macro_precision = precision_score(y_dev, y_pred, average = 'weighted')\n",
    "micro_recall = recall_score(y_dev, y_pred, average = 'micro')\n",
    "weighted_macro_recall = recall_score(y_dev, y_pred, average = 'weighted')\n",
    "micro_f1 = f1_score(y_dev, y_pred, average = 'micro')\n",
    "weighted_macro_f1 = f1_score(y_dev, y_pred, average = 'weighted')\n",
    "print(\"The micro precision is\", micro_precision)\n",
    "print(\"The weighted macro precision is\", weighted_macro_precision)\n",
    "print(\"The micro recall is\", micro_recall)\n",
    "print(\"The weighted macro recall is\", weighted_macro_recall)\n",
    "print(\"The micro f1 is\", micro_f1)\n",
    "print(\"The weighted macro f1 is\", weighted_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_USE_FC_samples(df_dict):\n",
    "\n",
    "    model_num = 1\n",
    "    preds_group = {}\n",
    "\n",
    "    for i in list(df_dict.values()):\n",
    "        print(\"Training model \" + str(model_num))\n",
    "        x_train = np.stack(i[\"USE_token_plot_embeddings\"])\n",
    "        preds_group[\"preds_{}\".format(model_num)] = train_USE_FC_model(x_train,\n",
    "                                                               \"./local/USE_fulldata_300dim_500batch_dropout_sample_{}.h5\".format(model_num),\n",
    "                                                                         i)\n",
    "        model_num += 1\n",
    "        \n",
    "    return preds_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 5s 45us/step - loss: 0.4340 - acc: 0.8034\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3942 - acc: 0.8207\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3912 - acc: 0.8223\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3895 - acc: 0.8233\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 3s 23us/step - loss: 0.3883 - acc: 0.8240\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3877 - acc: 0.8242\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3868 - acc: 0.8247\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3863 - acc: 0.8250\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3857 - acc: 0.8252\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3852 - acc: 0.8255\n",
      "Predicting\n",
      "Training model 2\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 5s 48us/step - loss: 0.4349 - acc: 0.8031\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3940 - acc: 0.8209\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3904 - acc: 0.8229\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3889 - acc: 0.8235\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 2s 22us/step - loss: 0.3877 - acc: 0.8242\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3870 - acc: 0.8245\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3861 - acc: 0.8249\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3855 - acc: 0.8255\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3852 - acc: 0.8255\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 2s 21us/step - loss: 0.3842 - acc: 0.8262\n",
      "Predicting\n",
      "Training model 3\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 5s 45us/step - loss: 0.4335 - acc: 0.8036\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 2s 21us/step - loss: 0.3939 - acc: 0.8210\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 2s 22us/step - loss: 0.3907 - acc: 0.8223\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3891 - acc: 0.8235\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3882 - acc: 0.8239\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 2s 22us/step - loss: 0.3872 - acc: 0.8242\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3863 - acc: 0.8250\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 2s 22us/step - loss: 0.3858 - acc: 0.8251\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 2s 21us/step - loss: 0.3851 - acc: 0.8254\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 2s 22us/step - loss: 0.3848 - acc: 0.8254\n",
      "Predicting\n",
      "Training model 4\n",
      "Epoch 1/10\n",
      "114730/114730 [==============================] - 5s 44us/step - loss: 0.4347 - acc: 0.8035\n",
      "Epoch 2/10\n",
      "114730/114730 [==============================] - 2s 21us/step - loss: 0.3940 - acc: 0.8210\n",
      "Epoch 3/10\n",
      "114730/114730 [==============================] - 3s 24us/step - loss: 0.3908 - acc: 0.8227\n",
      "Epoch 4/10\n",
      "114730/114730 [==============================] - 3s 27us/step - loss: 0.3890 - acc: 0.8233\n",
      "Epoch 5/10\n",
      "114730/114730 [==============================] - 3s 22us/step - loss: 0.3881 - acc: 0.8241\n",
      "Epoch 6/10\n",
      "114730/114730 [==============================] - 2s 22us/step - loss: 0.3871 - acc: 0.8246\n",
      "Epoch 7/10\n",
      "114730/114730 [==============================] - 2s 21us/step - loss: 0.3864 - acc: 0.8250\n",
      "Epoch 8/10\n",
      "114730/114730 [==============================] - 2s 20us/step - loss: 0.3857 - acc: 0.8254\n",
      "Epoch 9/10\n",
      "114730/114730 [==============================] - 2s 19us/step - loss: 0.3851 - acc: 0.8255\n",
      "Epoch 10/10\n",
      "114730/114730 [==============================] - 2s 18us/step - loss: 0.3846 - acc: 0.8258\n",
      "Predicting\n",
      "Training model 5\n"
     ]
    }
   ],
   "source": [
    "USE_FC_samples_preds_dict = train_USE_FC_samples(sample_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_pr_ensemble(y_dev, USE_FC_samples_preds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_class_scores_ensemble(y_dev, USE_FC_samples_preds_dict, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01384354, 0.02507908, 0.8756969 , 0.01674592, 0.10674362,\n",
       "       0.05267188, 0.02080834, 0.09573961, 0.00467977, 0.02327636],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparePred(pred, actual, dev_set, index):\n",
    "    genres = ['Action', 'Adventure', 'Comedy', 'Crime', 'Drama', 'Family',\n",
    "       'Horror', 'Romance', 'Sci-Fi', 'Thriller']\n",
    "    print(\"Predicted:\")\n",
    "    genreSelected = False\n",
    "    for i in range(10):\n",
    "        if(pred[index][i] >= 0.5):\n",
    "            print(genres[i])\n",
    "            print(pred[index][i])\n",
    "            genreSelected = True\n",
    "    if genreSelected ==False:\n",
    "        print(genres[np.argmax(pred[index])])\n",
    "        print(pred[index][i])\n",
    "\n",
    "    print()\n",
    "    print(\"Actual:\")\n",
    "    for i in range(10):\n",
    "        if actual[index][i] == 1:\n",
    "            print(genres[i])\n",
    "    print()\n",
    "    print(\"Plot:\")\n",
    "    print(dev_set.iloc[index][\"plots\"])\n",
    "    \n",
    "    print()\n",
    "    print(\"Tokens\")\n",
    "    #print(dev_set.iloc[index][\"USE_tokens_sentences_padded\"])\n",
    "    print(dev_set.iloc[index][\"tokenized_words\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "Comedy\n",
      "0.8756969\n",
      "\n",
      "Actual:\n",
      "Comedy\n",
      "\n",
      "Plot:\n",
      "Follows Manager Klaus Schlegle and the door-to-door salesmen of At Your Door Service Enterprises (A.Y.D.S.E.). New salesman Bryan Cooper, socially confused Cynthia Patterson, and fitness aficionado Ed Phuket are the sales team. Under the direction of their overly enthusiastic boss Klaus, and with sales routes mapped by bitter receptionist Mildred Mortimer, they go door to door to sell a variety of wacky products. \n",
      "\n",
      "Tokens\n",
      "['follows manager klaus schlegle and the door to door salesmen of at your door service enterprises aydse.', 'new salesman bryan cooper socially confused cynthia patterson and fitness aficionado ed phuket are the sales team.', 'under the direction of their overly enthusiastic boss klaus and with sales routes mapped by bitter receptionist mildred mortimer they go door to door to sell a variety of wacky products.', 'EMPTY', 'EMPTY']\n"
     ]
    }
   ],
   "source": [
    "comparePred(preds, y_dev, dev_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "Drama\n",
      "0.57791966\n",
      "\n",
      "Actual:\n",
      "Horror\n",
      "Sci-Fi\n",
      "\n",
      "Plot:\n",
      "Theodora, Jody and Carol, collectively The Violas, are on tour when their van breaks down in a small southern beach town. The local police are investigating a mass of mysterious debris on the beach and the disappearance of a little girl's parents after she's found walking the beach in a state of shock. Scientist John Patterson is called in to help investigate. Both John and local mechanic Hector Garcia fall for Violas lead singer Theodora, but she seems to have no interest in them, possibly because of a mysterious past. Hector convinces the girls to stay in town when he offers to repair their van in exchange for playing his party, but the mysterious creature is still on the loose. \n",
      "\n",
      "Tokens\n",
      "['theodora jody and carol collectively the violas are on tour when their van breaks down in a small southern beach town.', \"the local police are investigating a mass of mysterious debris on the beach and the disappearance of a little girl's parents after she's found walking the beach in a state of shock.\", 'scientist john patterson is called in to help investigate.', 'both john and local mechanic hector garcia fall for violas lead singer theodora but she seems to have no interest in them possibly because of a mysterious past.', 'hector convinces the girls to stay in town when he offers to repair their van in exchange for playing his party but the mysterious creature is still on the loose.']\n"
     ]
    }
   ],
   "source": [
    "comparePred(preds, y_dev, dev_set, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "Action\n",
      "0.5810109\n",
      "Drama\n",
      "0.5009569\n",
      "\n",
      "Actual:\n",
      "Action\n",
      "Thriller\n",
      "\n",
      "Plot:\n",
      "Gabriel is promised his freedom in return for his service in a rescue operation to save a missing British operative. When Gabriel is captured and taken to the compound where his target and several others are being held, he must devise a plan to take on the entire North Korean military installation and get everyone to safety. \n",
      "\n",
      "Tokens\n",
      "['gabriel is promised his freedom in return for his service in a rescue operation to save a missing british operative.', 'when gabriel is captured and taken to the compound where his target and several others are being held he must devise a plan to take on the entire north korean military installation and get everyone to safety.', 'EMPTY', 'EMPTY', 'EMPTY']\n"
     ]
    }
   ],
   "source": [
    "comparePred(preds, y_dev, dev_set, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "Drama\n",
      "0.914151\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "Moloya gets a shock of her life when her newly married husband Pranjal suddenly declares that his life ambition is to serve in the rural areas in Assam - something that was not known to her although they have known each other for many years and they have studied medicine together. The clash begins before they could start their home. \n",
      "\n",
      "Tokens\n",
      "['moloya gets a shock of her life when her newly married husband pranjal suddenly declares that his life ambition is to serve in the rural areas in assam something that was not known to her although they have known each other for many years and they have studied medicine together.', 'the clash begins before they could start their home.', 'EMPTY', 'EMPTY', 'EMPTY']\n"
     ]
    }
   ],
   "source": [
    "comparePred(preds, y_dev, dev_set, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "Drama\n",
      "0.65094334\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "What happens when a natural disaster hits a rural town of only 400 residents? In less than 6 hours Hurricane Irene destroyed all roads in and out of Pittsfield Vermont and the town became an isolated island with no power, phone service or medical facilities. In the face of chaos, would this small community fall apart or come together? \n",
      "\n",
      "Tokens\n",
      "['what happens when a natural disaster hits a rural town of only DG residents?', 'in less than DG hours hurricane irene destroyed all roads in and out of pittsfield vermont and the town became an isolated island with no power phone service or medical facilities.', 'in the face of chaos would this small community fall apart or come together?', 'EMPTY', 'EMPTY']\n"
     ]
    }
   ],
   "source": [
    "comparePred(preds, y_dev, dev_set, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "Comedy\n",
      "0.62210685\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "Romance\n",
      "\n",
      "Plot:\n",
      "Humans are known to be curious beasts. There are times when we all had fantasies that were so odd that we were afraid people would think we're crazy! But the women in this film have waited too long to get what they need. Veronica Vain is a newlywed, and her new mother-in-law Kendra James hears from her best friend Kendra Lust just how to get her into trouble. When Kendra's invited over for dinner she takes the real chance that she and Veronica might almost be caught in the act. Adriana Chechik is just like any other teen. She waits until her parents are out and then rushes to the family computer to look up all the weird and wacky fantasies running through her mind. Little does she know that her seemingly typical stepmom Veronica Avluv is actually an expert. Talking to her is like finding the holy grail when it comes to her biggest fantasy, squirting! \n",
      "\n",
      "Tokens\n",
      "['humans are known to be curious beasts.', \"there are times when we all had fantasies that were so odd that we were afraid people would think we're crazy!\", 'but the women in this film have waited too long to get what they need.', 'veronica vain is a newlywed and her new mother in law kendra james hears from her best friend kendra lust just how to get her into trouble.', \"when kendra's invited over for dinner she takes the real chance that she and veronica might almost be caught in the act.\"]\n"
     ]
    }
   ],
   "source": [
    "comparePred(preds, y_dev, dev_set, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "Drama\n",
      "0.12039418\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "A jaded, wealthy couple watch a blue movie in their castle home along with her adult son. The son is testy, so they go into town and watch a circus-like thrill ride. The daredevil woman in the show looks exactly like one of the women in the movie, so the man invites her to join them for a nightcap. Tensions among the family seem to rise. She stays overnight, and during her 24 hours in the castle, each of its three residents involves her in a fantasy. She, in turn, keeps asking, \"Who has the gun?\" Will there be violence before it's over? \n",
      "\n",
      "Tokens\n",
      "['a jaded wealthy couple watch a blue movie in their castle home along with her adult son.', 'the son is testy so they go into town and watch a circus like thrill ride.', 'the daredevil woman in the show looks exactly like one of the women in the movie so the man invites her to join them for a nightcap.', 'tensions among the genre seem to rise.', 'she stays overnight and during her DG hours in the castle each of its three residents involves her in a fantasy.']\n"
     ]
    }
   ],
   "source": [
    "comparePred(preds, y_dev, dev_set, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impractical to create a confusion matrix between all possible outcomes since this is a multilabel problem so instead we will just build it for the set of 1 or 2 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConfusionMatrix(pred, actual, dev_set, alwaysPredict = False):\n",
    "    genres = ['Action', 'Adventure', 'Comedy', 'Crime', 'Drama', 'Family',\n",
    "       'Horror', 'Romance', 'Sci-Fi', 'Thriller']\n",
    "    a = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '01', '02', '03', '04', '05','06',\n",
    "                 '07', '08', '09', '12' ,'13', '14', '15', '16', '17', '18', '19', '23', '24', '25', '26', '27',\n",
    "                 '28','29','34', '35','36','37','38','39','45','46','47','48','49','56','57','58','59','67',\n",
    "                 '68','69','78','79','89','99'])\n",
    "    matrix = np.zeros((len(a),len(a)))\n",
    "    for index in range(len(pred)):\n",
    "        genreSelected = False\n",
    "        predictions = np.zeros((10))\n",
    "        #print(\"hi\")\n",
    "        predictedIndices = \"\"\n",
    "        for i in range(10):\n",
    "            if(pred[index][i] >= 0.5):\n",
    "                predictions[i] = 1\n",
    "                genreSelected = True\n",
    "                predictedIndices += str(i)\n",
    "                #print(predictedIndices)\n",
    "        if alwaysPredict and genreSelected ==False:\n",
    "            predictions[np.argmax(pred[index])] = 1\n",
    "            predictedIndices = str(np.argmax(pred[index]))\n",
    "            genreSelected = True\n",
    "        if genreSelected == False:\n",
    "            predictedIndices = '99'\n",
    "        \n",
    "        actualIndices = \"\"\n",
    "        for k in range(10):\n",
    "            if actual[index][k] == 1:\n",
    "                actualIndices += str(k)\n",
    "                #print(actualIndices)\n",
    "        if(len(predictedIndices)<3 and len(actualIndices)<3):\n",
    "            #print(predictedIndices)\n",
    "            #print(actualIndices)\n",
    "            column = np.argwhere(a==predictedIndices)[0][0]\n",
    "            row = np.argwhere(a==actualIndices)[0][0]\n",
    "            matrix[row,column] = matrix[row,column] + 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = createConfusionMatrix(preds, y_dev, dev_set, alwaysPredict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  96.,    4.,   23.,    0.,  122.,    0.,    3.,    0.,   13.,\n",
       "           1.],\n",
       "       [  13.,   56.,   41.,    0.,  110.,    3.,    6.,    0.,   11.,\n",
       "           1.],\n",
       "       [  13.,    4., 1751.,   10.,  695.,    6.,   32.,    2.,   10.,\n",
       "           1.],\n",
       "       [   5.,    0.,   13.,   11.,  105.,    0.,    2.,    0.,    0.,\n",
       "           5.],\n",
       "       [  14.,   10.,  375.,    4., 4294.,   15.,   23.,    0.,   11.,\n",
       "           4.],\n",
       "       [   1.,    8.,   86.,    0.,  224.,   46.,    3.,    0.,    4.,\n",
       "           0.],\n",
       "       [   6.,    2.,   75.,    3.,  156.,    1.,  197.,    0.,    5.,\n",
       "           9.],\n",
       "       [   1.,    0.,   50.,    1.,  150.,    1.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   5.,    0.,   18.,    0.,   64.,    0.,    7.,    0.,   93.,\n",
       "           4.],\n",
       "       [   4.,    1.,   36.,    6.,  235.,    0.,   19.,    0.,    4.,\n",
       "          19.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0:10,0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE HAN ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.stack(test_set[\"USE_token_sentence_embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_1.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model2 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_2.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model3 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_3.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model4 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_4.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model5 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_5.h5', custom_objects={'AttentionWithContext': AttentionWithContext})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict(x_test)\n",
    "pred2 = model2.predict(x_test)\n",
    "pred3 = model3.predict(x_test)\n",
    "pred4 = model4.predict(x_test)\n",
    "pred5 = model5.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEnsembleConfusionMatrix(pred1, pred2, pred3, pred4, pred5, actual, dev_set, alwaysPredict = False):\n",
    "    genres = ['Action', 'Adventure', 'Comedy', 'Crime', 'Drama', 'Family',\n",
    "       'Horror', 'Romance', 'Sci-Fi', 'Thriller']\n",
    "    a = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '01', '02', '03', '04', '05','06',\n",
    "                 '07', '08', '09', '12' ,'13', '14', '15', '16', '17', '18', '19', '23', '24', '25', '26', '27',\n",
    "                 '28','29','34', '35','36','37','38','39','45','46','47','48','49','56','57','58','59','67',\n",
    "                 '68','69','78','79','89','99'])\n",
    "    d={}\n",
    "    matrix = np.zeros((len(a),len(a)))\n",
    "    pred = pred1+pred2+pred3+pred4+pred5\n",
    "    good_plot =0\n",
    "    good_num_sent_total =0\n",
    "    good_sent_len_total =0\n",
    "    good_truncated = 0\n",
    "    bad_plot =0\n",
    "    bad_num_sent_total =0\n",
    "    bad_sent_len_total =0\n",
    "    bad_truncated = 0\n",
    "    for index in range(len(pred)):\n",
    "        genreSelected = False\n",
    "        predictions = np.zeros((10))\n",
    "        #print(\"hi\")\n",
    "        predictedIndices = \"\"\n",
    "        for i in range(10):\n",
    "            if(pred[index][i] >= 2.5):\n",
    "                predictions[i] = 1\n",
    "                genreSelected = True\n",
    "                predictedIndices += str(i)\n",
    "                #print(predictedIndices)\n",
    "        if alwaysPredict and genreSelected ==False:\n",
    "            predictions[np.argmax(pred[index])] = 1\n",
    "            predictedIndices = str(np.argmax(pred[index]))\n",
    "            genreSelected = True\n",
    "        if genreSelected == False:\n",
    "            predictedIndices = '99'\n",
    "        \n",
    "        actualIndices = \"\"\n",
    "        for k in range(10):\n",
    "            if actual[index][k] == 1:\n",
    "                actualIndices += str(k)\n",
    "                #print(actualIndices)\n",
    "        if(len(predictedIndices)<3 and len(actualIndices)<3):\n",
    "            #print(predictedIndices)\n",
    "            #print(actualIndices)\n",
    "            column = np.argwhere(a==predictedIndices)[0][0]\n",
    "            row = np.argwhere(a==actualIndices)[0][0]\n",
    "            matrix[row,column] = matrix[row,column] + 1\n",
    "            \n",
    "        tokenized_words = dev_set.iloc[index][\"tokenized_words\"]\n",
    "        num_sent = len(tokenized_words)\n",
    "        sent_len = 0\n",
    "        for sent in range(num_sent):\n",
    "            sent_len += len(tokenized_words[sent])\n",
    "        avg_sent_len = sent_len/num_sent\n",
    "        if(num_sent>5):\n",
    "            truncated_sent = True\n",
    "        else:\n",
    "            truncated_sent = False\n",
    "        if predictedIndices == actualIndices:\n",
    "            good_plot +=1\n",
    "            good_num_sent_total += num_sent\n",
    "            good_sent_len_total += avg_sent_len\n",
    "            if truncated_sent:\n",
    "                good_truncated +=1\n",
    "        else:\n",
    "            bad_plot +=1\n",
    "            bad_num_sent_total += num_sent\n",
    "            bad_sent_len_total += avg_sent_len\n",
    "            if truncated_sent:\n",
    "                bad_truncated +=1\n",
    "        if(len(predictedIndices)<2 and len(actualIndices)<2):\n",
    "            if actualIndices not in d:\n",
    "                d[actualIndices] = [1, num_sent,avg_sent_len]\n",
    "            else:\n",
    "                d[actualIndices][0] += 1\n",
    "                d[actualIndices][1] += num_sent\n",
    "                d[actualIndices][2] += avg_sent_len\n",
    "                \n",
    "    for key, value in d.items():\n",
    "        print(genres[int(key)])\n",
    "        print(\"Avg sent number\")\n",
    "        print(value[1]/value[0])\n",
    "        print(\"Avg sentence length\")\n",
    "        print(value[2]/value[0])\n",
    "        \n",
    "    print(\"Good plots:\")\n",
    "    print(\"Avg sent number\")\n",
    "    print(good_num_sent_total/good_plot)\n",
    "    print(\"Avg sentence length\")\n",
    "    print(good_sent_len_total/good_plot)\n",
    "    print(\"Fraction sentence truncated\")\n",
    "    print(good_truncated/good_plot)\n",
    "    print(\"Bad plots:\")\n",
    "    print(\"Avg sent number\")\n",
    "    print(bad_num_sent_total/bad_plot)\n",
    "    print(\"Avg sentence length\")\n",
    "    print(bad_sent_len_total/bad_plot)\n",
    "    print(\"Fraction sentence truncated\")\n",
    "    print(bad_truncated/bad_plot)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_set.iloc[0][\"tokenized_words\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sampled = createEnsembleConfusionMatrix(pred1, pred2, pred3, pred4, pred5, y_dev, dev_set, alwaysPredict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 102.,    9.,    8.,    7.,   36.,    6.,    5.,    0.,   10.,\n",
       "           2.],\n",
       "       [  13.,  123.,   22.,    1.,   37.,   16.,   13.,    0.,   15.,\n",
       "           1.],\n",
       "       [  27.,   21., 1293.,   43.,  388.,   40.,   75.,    9.,   31.,\n",
       "          11.],\n",
       "       [   6.,    1.,    6.,   22.,   33.,    2.,    5.,    0.,    1.,\n",
       "           6.],\n",
       "       [  37.,   78.,  246.,   25., 2560.,  104.,   62.,   14.,   47.,\n",
       "          38.],\n",
       "       [   2.,   31.,   32.,    0.,   77.,  174.,    9.,    0.,    3.,\n",
       "           0.],\n",
       "       [   5.,    3.,   27.,    3.,   56.,    3.,  252.,    1.,   10.,\n",
       "          14.],\n",
       "       [   3.,    3.,   23.,    2.,   57.,    3.,    0.,    3.,    0.,\n",
       "           1.],\n",
       "       [   3.,    4.,    3.,    0.,   26.,    1.,    8.,    3.,  124.,\n",
       "           5.],\n",
       "       [   2.,    4.,   14.,   13.,   99.,    0.,   28.,    0.,    7.,\n",
       "          33.]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_sampled[0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sampled = createEnsembleConfusionMatrix(pred1, pred2, pred3, pred4, pred5, y_dev, dev_set, alwaysPredict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 108.,   15.,    8.,    9.,   50.,    9.,    5.,    0.,   12.,\n",
       "           5.],\n",
       "       [  17.,  139.,   28.,    8.,   50.,   23.,   15.,    2.,   16.,\n",
       "           5.],\n",
       "       [  47.,   39., 1437.,   50.,  518.,   66.,   84.,   13.,   43.,\n",
       "          32.],\n",
       "       [   6.,    3.,    9.,   28.,   40.,    3.,    5.,    0.,    1.,\n",
       "           8.],\n",
       "       [  56.,  117.,  326.,   31., 2852.,  149.,   84.,   16.,   59.,\n",
       "          62.],\n",
       "       [   4.,   41.,   44.,    0.,  106.,  204.,   12.,    0.,    5.,\n",
       "           2.],\n",
       "       [  10.,    9.,   30.,    3.,   74.,    5.,  261.,    1.,   13.,\n",
       "          24.],\n",
       "       [   3.,    7.,   32.,    2.,   70.,    6.,    2.,    6.,    1.,\n",
       "           1.],\n",
       "       [   7.,    7.,    7.,    1.,   33.,    4.,   12.,    3.,  132.,\n",
       "          10.],\n",
       "       [   5.,    4.,   26.,   13.,  115.,    0.,   36.,    1.,    9.,\n",
       "          45.]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_sampled[0:10,0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine most common tokens per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Action', 'Adventure', 'Comedy', 'Crime', 'Drama', 'Family',\n",
    "       'Horror', 'Romance', 'Sci-Fi', 'Thriller']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTopWordCount(pred, actual, dev_set, alwaysPredict = False):\n",
    "    genres = ['Action', 'Adventure', 'Comedy', 'Crime', 'Drama', 'Family',\n",
    "       'Horror', 'Romance', 'Sci-Fi', 'Thriller']\n",
    "    a = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '01', '02', '03', '04', '05','06',\n",
    "                 '07', '08', '09', '12' ,'13', '14', '15', '16', '17', '18', '19', '23', '24', '25', '26', '27',\n",
    "                 '28','29','34', '35','36','37','38','39','45','46','47','48','49','56','57','58','59','67',\n",
    "                 '68','69','78','79','89','99'])\n",
    "    matrix = np.zeros((len(a),len(a)))\n",
    "    pred_word_dict = {'0':{}, '1':{}, '2':{}, '3':{}, '4':{}, '5':{}, '6':{}, '7':{}, '8':{}, '9':{}}\n",
    "    actual_word_dict = {'0':{}, '1':{}, '2':{}, '3':{}, '4':{}, '5':{}, '6':{}, '7':{}, '8':{}, '9':{}}\n",
    "    for index in range(len(pred)):\n",
    "        genreSelected = False\n",
    "        predictions = np.zeros((10))\n",
    "        #print(\"hi\")\n",
    "        predictedIndices = \"\"\n",
    "        for i in range(10):\n",
    "            if(pred[index][i] >= 0.5):\n",
    "                predictions[i] = 1\n",
    "                genreSelected = True\n",
    "                predictedIndices += str(i)\n",
    "                #print(predictedIndices)\n",
    "        if alwaysPredict and genreSelected ==False:\n",
    "            predictions[np.argmax(pred[index])] = 1\n",
    "            predictedIndices = str(np.argmax(pred[index]))\n",
    "            genreSelected = True\n",
    "        if genreSelected == False:\n",
    "            predictedIndices = '99'\n",
    "        \n",
    "        actualIndices = \"\"\n",
    "        for k in range(10):\n",
    "            if actual[index][k] == 1:\n",
    "                actualIndices += str(k)\n",
    "                #print(actualIndices)\n",
    "        \n",
    "        tokens = dev_set.iloc[index]['flattened_tokens']\n",
    "                \n",
    "        if predictedIndices in pred_word_dict:\n",
    "            #for each word in plot, add or update in dictionary\n",
    "            for token in tokens:\n",
    "                if token in pred_word_dict[predictedIndices]:\n",
    "                    pred_word_dict[predictedIndices][token] +=1\n",
    "                else:\n",
    "                    pred_word_dict[predictedIndices][token] = 1\n",
    "        \n",
    "        if actualIndices in actual_word_dict:\n",
    "            #for each word in plot, add or update in dictionary\n",
    "            for token in tokens:\n",
    "                if token in actual_word_dict[actualIndices]:\n",
    "                    actual_word_dict[actualIndices][token] +=1\n",
    "                else:\n",
    "                    actual_word_dict[actualIndices][token] = 1\n",
    "                \n",
    "        \n",
    "        if(len(predictedIndices)<3 and len(actualIndices)<3):\n",
    "            #print(predictedIndices)\n",
    "            #print(actualIndices)\n",
    "            column = np.argwhere(a==predictedIndices)[0][0]\n",
    "            row = np.argwhere(a==actualIndices)[0][0]\n",
    "            matrix[row,column] = matrix[row,column] + 1\n",
    "    \n",
    "    \n",
    "    for key in pred_word_dict:\n",
    "        print(genres[int(key)])\n",
    "        print(\"Predictions\")\n",
    "        A = pred_word_dict[key]\n",
    "        print(sorted(A, key=A.get, reverse=True)[:5])\n",
    "        print(\"Actual\")\n",
    "        B = actual_word_dict[key]\n",
    "        print(sorted(B, key=B.get, reverse=True)[:5])\n",
    "        print()\n",
    "    \n",
    "    return actual_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\n",
      "Predictions\n",
      "['.', \"'s\", 'DG', 'genre', 'one']\n",
      "Actual\n",
      "['.', \"'s\", 'genre', 'DG', 'vs.']\n",
      "\n",
      "Adventure\n",
      "Predictions\n",
      "['.', \"'s\", 'DG', 'world', 'genre']\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'world', 'genre']\n",
      "\n",
      "Comedy\n",
      "Predictions\n",
      "['.', \"'s\", 'get', 'genre', 'DG']\n",
      "Actual\n",
      "['.', \"'s\", 'get', 'genre', 'one']\n",
      "\n",
      "Crime\n",
      "Predictions\n",
      "['.', \"'s\", 'murder', 'genre', \"'\"]\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'genre', 'police']\n",
      "\n",
      "Drama\n",
      "Predictions\n",
      "['.', \"'s\", 'life', 'DG', 'genre']\n",
      "Actual\n",
      "['.', \"'s\", 'life', 'DG', 'one']\n",
      "\n",
      "Family\n",
      "Predictions\n",
      "['.', \"'s\", 'DG', 'child', 'genre']\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'genre', 'life']\n",
      "\n",
      "Horror\n",
      "Predictions\n",
      "['.', \"'s\", 'genre', 'find', 'one']\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'genre', 'one']\n",
      "\n",
      "Romance\n",
      "Predictions\n",
      "['.', \"'s\", 'love', 'jerry', 'ca']\n",
      "Actual\n",
      "['.', \"'s\", 'love', 'life', 'get']\n",
      "\n",
      "Sci-Fi\n",
      "Predictions\n",
      "['.', \"'s\", 'DG', 'earth', 'world']\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'world', 'one']\n",
      "\n",
      "Thriller\n",
      "Predictions\n",
      "['.', \"'s\", 'find', 'one', 'get']\n",
      "Actual\n",
      "['.', \"'s\", 'find', 'life', 'one']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_word_dict = createTopWordCount(pred, y_dev, dev_set, alwaysPredict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\n",
      "Predictions\n",
      "['.', \"'s\", 'DG', 'genre', 'one', 'vs.', 'take', 'world', 'fight', 'get']\n",
      "Actual\n",
      "['.', \"'s\", 'genre', 'DG', 'vs.', 'find', 'one', 'take', 'world', 'get']\n",
      "\n",
      "Adventure\n",
      "Predictions\n",
      "['.', \"'s\", 'DG', 'world', 'genre', 'journey', 'story', 'take', 'one', 'film']\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'world', 'genre', 'one', 'story', 'life', 'find', 'film']\n",
      "\n",
      "Comedy\n",
      "Predictions\n",
      "['.', \"'s\", 'get', 'genre', 'DG', 'one', 'make', 'take', \"'\", 'find']\n",
      "Actual\n",
      "['.', \"'s\", 'get', 'genre', 'one', 'DG', 'life', 'find', 'make', 'go']\n",
      "\n",
      "Crime\n",
      "Predictions\n",
      "['.', \"'s\", 'murder', 'genre', \"'\", 'story', 'police', 'one', 'DG', 'friend']\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'genre', 'police', 'one', 'film', 'murder', 'get', 'life']\n",
      "\n",
      "Drama\n",
      "Predictions\n",
      "['.', \"'s\", 'life', 'DG', 'genre', 'one', 'young', 'story', 'find', 'film']\n",
      "Actual\n",
      "['.', \"'s\", 'life', 'DG', 'one', 'genre', 'young', 'story', 'love', 'film']\n",
      "\n",
      "Family\n",
      "Predictions\n",
      "['.', \"'s\", 'DG', 'child', 'genre', 'world', '!', 'kid', 'one', 'year']\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'genre', 'life', \"'\", 'film', 'child', 'year', 'story']\n",
      "\n",
      "Horror\n",
      "Predictions\n",
      "['.', \"'s\", 'genre', 'find', 'one', 'DG', 'take', 'house', '?', 'film']\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'genre', 'one', 'find', 'night', 'take', 'go', 'young']\n",
      "\n",
      "Romance\n",
      "Predictions\n",
      "['.', \"'s\", 'love', 'jerry', 'ca', 'go', 'beautiful', 'woman', 'find', 'life']\n",
      "Actual\n",
      "['.', \"'s\", 'love', 'life', 'get', 'meet', 'DG', 'genre', 'girl', 'find']\n",
      "\n",
      "Sci-Fi\n",
      "Predictions\n",
      "['.', \"'s\", 'DG', 'earth', 'world', 'find', 'space', 'alien', 'one', 'human']\n",
      "Actual\n",
      "['.', \"'s\", 'DG', 'world', 'one', 'find', 'life', '?', 'human', 'take']\n",
      "\n",
      "Thriller\n",
      "Predictions\n",
      "['.', \"'s\", 'find', 'one', 'get', 'life', 'man', 'genre', 'go', '?']\n",
      "Actual\n",
      "['.', \"'s\", 'find', 'life', 'one', 'genre', 'get', 'man', '?', 'young']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in pred_word_dict:\n",
    "        print(genres[int(key)])\n",
    "        print(\"Predictions\")\n",
    "        A = pred_word_dict[key]\n",
    "        print(sorted(A, key=A.get, reverse=True)[:10])\n",
    "        print(\"Actual\")\n",
    "        B = actual_word_dict[key]\n",
    "        print(sorted(B, key=B.get, reverse=True)[:10])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load previously trained ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.stack(test_set['USE_token_plot_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('./local/USE_fulldata_300dim_500batch_dropout_sample_1.h5')#, custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model2 = load_model('./local/USE_fulldata_300dim_500batch_dropout_sample_2.h5')#, custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model3 = load_model('./local/USE_fulldata_300dim_500batch_dropout_sample_3.h5')#, custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model4 = load_model('./local/USE_fulldata_300dim_500batch_dropout_sample_4.h5')#, custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model5 = load_model('./local/USE_fulldata_300dim_500batch_dropout_sample_5.h5')#, custom_objects={'AttentionWithContext': AttentionWithContext})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict(x_test)\n",
    "pred2 = model2.predict(x_test)\n",
    "pred3 = model3.predict(x_test)\n",
    "pred4 = model4.predict(x_test)\n",
    "pred5 = model5.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_group={}\n",
    "preds_group[\"preds_1\"]=pred1\n",
    "preds_group[\"preds_2\"]=pred2\n",
    "preds_group[\"preds_3\"]=pred3\n",
    "preds_group[\"preds_4\"]=pred4\n",
    "preds_group[\"preds_5\"]=pred5\n",
    "USE_FC_preds = preds_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for Action is 0.529896013864818\n",
      "The precision for Adventure is 0.44788164088769333\n",
      "The precision for Comedy is 0.6786512988130053\n",
      "The precision for Crime is 0.41795511221945136\n",
      "The precision for Drama is 0.7068352429869617\n",
      "The precision for Family is 0.43494038361845516\n",
      "The precision for Horror is 0.5747716894977168\n",
      "The precision for Romance is 0.4023732470334412\n",
      "The precision for Sci-Fi is 0.5592322964923891\n",
      "The precision for Thriller is 0.45951417004048584\n",
      "The recall for Action is 0.5037067545304778\n",
      "The recall for Adventure is 0.3673469387755102\n",
      "The recall for Comedy is 0.5572821019918067\n",
      "The recall for Crime is 0.46633277685030605\n",
      "The recall for Drama is 0.7608233392872331\n",
      "The recall for Family is 0.4213962832747363\n",
      "The recall for Horror is 0.5212215320910973\n",
      "The recall for Romance is 0.3987883107626515\n",
      "The recall for Sci-Fi is 0.6132075471698113\n",
      "The recall for Thriller is 0.31451333564253553\n",
      "The f1 for Action is 0.5164695945945945\n",
      "The f1 for Adventure is 0.4036363636363637\n",
      "The f1 for Comedy is 0.6120074464784362\n",
      "The f1 for Crime is 0.44082062072593375\n",
      "The f1 for Drama is 0.7328363100114696\n",
      "The f1 for Family is 0.4280612244897959\n",
      "The f1 for Horror is 0.5466883821932681\n",
      "The f1 for Romance is 0.4005727581886522\n",
      "The f1 for Sci-Fi is 0.5849775008653514\n",
      "The f1 for Thriller is 0.3734320378367263\n",
      "The micro precision is 0.5942952333635327\n",
      "The weighted macro precision is 0.590131058935253\n",
      "The micro recall is 0.5669398907103825\n",
      "The weighted macro recall is 0.5669398907103825\n",
      "The micro f1 is 0.5802953556395806\n",
      "The weighted macro f1 is 0.5755260697579909\n",
      "The average number of genres per movie is 1.5254335517810174\n",
      "The median number of genres per movie is 1.0\n",
      "The AUC for Action is 0.5302643780935521\n",
      "The AUC for Adventure is 0.38174256746953006\n",
      "The AUC for Comedy is 0.6827340234738716\n",
      "The AUC for Crime is 0.39846073372619206\n",
      "The AUC for Drama is 0.7734685258393357\n",
      "The AUC for Family is 0.40341673739027806\n",
      "The AUC for Horror is 0.5711999084152659\n",
      "The AUC for Romance is 0.36598090621055845\n",
      "The AUC for Sci-Fi is 0.6175446419140816\n",
      "The AUC for Thriller is 0.3928507342188565\n",
      "The micro-avg AUC is 0.5959013941625186\n",
      "The macro-avg AUC is 0.5117663156751522\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores_ensemble(y_test, preds_group, 0.5)\n",
    "auc_pr_ensemble(y_test, preds_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.stack(test_set['USE_token_sentence_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_1.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model2 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_2.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model3 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_3.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model4 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_4.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model5 = load_model('./local/USE_HAN_300dim_500batch_dropout_sample_5.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict(x_test)\n",
    "pred2 = model2.predict(x_test)\n",
    "pred3 = model3.predict(x_test)\n",
    "pred4 = model4.predict(x_test)\n",
    "pred5 = model5.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_group={}\n",
    "preds_group[\"preds_1\"]=pred1\n",
    "preds_group[\"preds_2\"]=pred2\n",
    "preds_group[\"preds_3\"]=pred3\n",
    "preds_group[\"preds_4\"]=pred4\n",
    "preds_group[\"preds_5\"]=pred5\n",
    "USE_HAN_preds = preds_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for Action is 0.44673847995212446\n",
      "The precision for Adventure is 0.3742430359305612\n",
      "The precision for Comedy is 0.6175303595046291\n",
      "The precision for Crime is 0.3449660284126004\n",
      "The precision for Drama is 0.6830112888704204\n",
      "The precision for Family is 0.3277777777777778\n",
      "The precision for Horror is 0.4844511052828775\n",
      "The precision for Romance is 0.3248910110026988\n",
      "The precision for Sci-Fi is 0.49396984924623116\n",
      "The precision for Thriller is 0.3842691305377873\n",
      "The recall for Action is 0.6149093904448105\n",
      "The recall for Adventure is 0.5113072255929398\n",
      "The recall for Comedy is 0.7255262042661393\n",
      "The recall for Crime is 0.6215915414579856\n",
      "The recall for Drama is 0.8388194267245046\n",
      "The recall for Family is 0.6223003515821195\n",
      "The recall for Horror is 0.6692546583850931\n",
      "The recall for Romance is 0.5577334283677833\n",
      "The recall for Sci-Fi is 0.7133526850507983\n",
      "The recall for Thriller is 0.5618288881191549\n",
      "The f1 for Action is 0.5175043327556326\n",
      "The f1 for Adventure is 0.43216783216783217\n",
      "The f1 for Comedy is 0.6671862821512081\n",
      "The f1 for Crime is 0.4436941410129096\n",
      "The f1 for Drama is 0.7529393800580241\n",
      "The f1 for Family is 0.42938832091491946\n",
      "The f1 for Horror is 0.5620517278852424\n",
      "The f1 for Romance is 0.4105995015085924\n",
      "The f1 for Sci-Fi is 0.583729216152019\n",
      "The f1 for Thriller is 0.45638716938660673\n",
      "The micro precision is 0.5120105498072631\n",
      "The weighted macro precision is 0.5318120175272013\n",
      "The micro recall is 0.7036076725772276\n",
      "The weighted macro recall is 0.7036076725772276\n",
      "The micro f1 is 0.5927100213720378\n",
      "The weighted macro f1 is 0.6029766381751803\n",
      "The average number of genres per movie is 2.1974053764878962\n",
      "The median number of genres per movie is 2.0\n",
      "The AUC for Action is 0.535324738229539\n",
      "The AUC for Adventure is 0.436963242650528\n",
      "The AUC for Comedy is 0.7327045397251158\n",
      "The AUC for Crime is 0.40914317317677473\n",
      "The AUC for Drama is 0.7911893853220142\n",
      "The AUC for Family is 0.47735078714215984\n",
      "The AUC for Horror is 0.6021449234783465\n",
      "The AUC for Romance is 0.3658379210939353\n",
      "The AUC for Sci-Fi is 0.6499270027561107\n",
      "The AUC for Thriller is 0.4287621484883989\n",
      "The micro-avg AUC is 0.6252859707030586\n",
      "The macro-avg AUC is 0.5429347862062923\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores_ensemble(y_test, preds_group, 0.5)\n",
    "auc_pr_ensemble(y_test, preds_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.stack(test_set[\"embedding\"])\n",
    "model1 = load_model('./local/HAN_300dim_500batch_dropout_sample_1.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model2 = load_model('./local/HAN_300dim_500batch_dropout_sample_2.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model3 = load_model('./local/HAN_300dim_500batch_dropout_sample_3.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model4 = load_model('./local/HAN_300dim_500batch_dropout_sample_4.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "model5 = load_model('./local/HAN_300dim_500batch_dropout_sample_5.h5', custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "pred1 = model1.predict(x_test)\n",
    "pred2 = model2.predict(x_test)\n",
    "pred3 = model3.predict(x_test)\n",
    "pred4 = model4.predict(x_test)\n",
    "pred5 = model5.predict(x_test)\n",
    "HAN_preds = preds_group\n",
    "indiv_class_scores_ensemble(y_test, preds_group, 0.5)\n",
    "auc_pr_ensemble(y_test, preds_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample of summaries and the predictions made by the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAN_sum = HAN_preds['preds_1'] + HAN_preds['preds_2'] + HAN_preds['preds_3'] + HAN_preds['preds_4'] + HAN_preds['preds_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_HAN_sum = USE_HAN_preds['preds_1'] + USE_HAN_preds['preds_2'] + USE_HAN_preds['preds_3'] + USE_HAN_preds['preds_4'] + USE_HAN_preds['preds_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FC_sum = USE_FC_preds['preds_1'] + USE_FC_preds['preds_2'] + USE_FC_preds['preds_3'] + USE_FC_preds['preds_4'] + USE_FC_preds['preds_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, actual, test_set, index = -1):\n",
    "    genres = ['Action', 'Adventure', 'Comedy', 'Crime', 'Drama', 'Family',\n",
    "       'Horror', 'Romance', 'Sci-Fi', 'Thriller']\n",
    "    if index == -1:\n",
    "        index = random.randint(0,test_set.shape[0])\n",
    "    print(\"Record \" + str(index))\n",
    "    print(\"Predicted:\")\n",
    "    print(\"HAN:\")\n",
    "    genreSelected = False\n",
    "    for i in range(10):\n",
    "        if(HAN_sum[index][i] >= 2.5):\n",
    "            print(genres[i])\n",
    "            print(HAN_sum[index][i]/5)\n",
    "            genreSelected = True\n",
    "    if genreSelected ==False:\n",
    "        print(genres[np.argmax(HAN_sum[index])])\n",
    "        print(HAN_sum[index][i]/5)\n",
    "    \n",
    "    print(\"USE + HAN:\")\n",
    "    genreSelected = False\n",
    "    for i in range(10):\n",
    "        if(USE_HAN_sum[index][i] >= 2.5):\n",
    "            print(genres[i])\n",
    "            print(USE_HAN_sum[index][i]/5)\n",
    "            genreSelected = True\n",
    "    if genreSelected ==False:\n",
    "        print(genres[np.argmax(USE_HAN_sum[index])])\n",
    "        print(USE_HAN_sum[index][i]/5)\n",
    "\n",
    "    print(\"USE + FC:\")\n",
    "    genreSelected = False\n",
    "    for i in range(10):\n",
    "        if(USE_FC_sum[index][i] >= 2.5):\n",
    "            print(genres[i])\n",
    "            print(USE_FC_sum[index][i]/5)\n",
    "            genreSelected = True\n",
    "    if genreSelected ==False:\n",
    "        print(genres[np.argmax(USE_FC_sum[index])])\n",
    "        print(USE_FC_sum[index][i]/5)\n",
    "        \n",
    "    print()\n",
    "    print(\"Actual:\")\n",
    "    for i in range(10):\n",
    "        if actual[index][i] == 1:\n",
    "            print(genres[i])\n",
    "    print()\n",
    "    print(\"Plot:\")\n",
    "    print(test_set.iloc[index][\"plots\"])\n",
    "    \n",
    "    #print()\n",
    "    #print(\"Tokens\")\n",
    "    #print(dev_set.iloc[index][\"USE_tokens_sentences_padded\"])\n",
    "    #print(dev_set.iloc[index][\"tokenized_words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1\n",
      "Predicted:\n",
      "HAN:\n",
      "Drama\n",
      "0.5080879688262939\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.5326623439788818\n",
      "USE + FC:\n",
      "Comedy\n",
      "0.12696411609649658\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "A couple and a dog. They live not far from Mexico City. Strange things happen in the house. She is in love. She'd like to be pregnant. He is busy and tired. The woman is undressing inside the house. He falls asleep. She pours wine on his face. A strange visitor dances outside. She stares at us, under the waterfall. They are imagining things. They might be happy in a near future. The film was inspired by a famous Edward Hopper painting, 'Summer evening'. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 18705\n",
      "Predicted:\n",
      "HAN:\n",
      "Drama\n",
      "0.9321073532104492\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.5588393211364746\n",
      "USE + FC:\n",
      "Drama\n",
      "0.5573939323425293\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "On the edge of a rocky desolate Mexican border town is a makeshift bar and brothel called \"Las Mujeres de Troya,\" whose only patrons are soldiers from the other side. Doa Hecuba, the bar's proprietress, routinely overlooks the preparations for the night shift. But this evening fate intervenes in the form of Agamemnon, a powerful General from across the border, who has sent for Doa Hecuba's uncorrupted, enigmatic daughter, Cassandra. Doa Hecuba's desperate pleas are powerless against Cassandra's mysterious and fateful volition, which compels her to submit to Agamemnon. Her decision to cross the border takes us beyond the barriers of the flesh to the very limits of perception. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 14\n",
      "Predicted:\n",
      "HAN:\n",
      "Thriller\n",
      "0.6639695644378663\n",
      "USE + HAN:\n",
      "Thriller\n",
      "0.46327505111694334\n",
      "USE + FC:\n",
      "Drama\n",
      "0.4540302276611328\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "Dr. Wayne is a brilliant surgeon. He is jealous of his friend Bowen, who may be paying too much attention to Pat, the surgeon's seductive spouse. When Wayne must perform an emergency operation, and the patient dies, he is suspected of murder. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 15\n",
      "Predicted:\n",
      "HAN:\n",
      "Drama\n",
      "0.8375716209411621\n",
      "Romance\n",
      "0.5462236881256104\n",
      "USE + HAN:\n",
      "Comedy\n",
      "0.5423326969146729\n",
      "Romance\n",
      "0.6655119895935059\n",
      "USE + FC:\n",
      "Romance\n",
      "0.5684175491333008\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "Romance\n",
      "\n",
      "Plot:\n",
      "Sadie is trapped in an abusive relationship with a man she can't help but love. After Jake falls through the ice of a river and drowns, Sadie, lost and alone in the woods, discovers duplicates of both herself and Jake as if nothing ever happened: but with a twist. This Jake is kind and loving. Could this be Sadie's chance at getting the life she's always wanted? \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "HAN:\n",
      "Comedy\n",
      "0.671903419494629\n",
      "USE + HAN:\n",
      "Comedy\n",
      "0.21234722137451173\n",
      "USE + FC:\n",
      "Drama\n",
      "0.26304049491882325\n",
      "\n",
      "Actual:\n",
      "Comedy\n",
      "\n",
      "Plot:\n",
      "When naive teen rapper Young Flaccid is shot at an awards show by his nemesis, the washed-up old school rapper Lil' Unkle, he is shocked to wake up in the hospital and find that his record company faked his death in order to sell more posthumous records. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "HAN:\n",
      "Horror\n",
      "0.9829551696777343\n",
      "USE + HAN:\n",
      "Horror\n",
      "0.8792496681213379\n",
      "USE + FC:\n",
      "Horror\n",
      "0.84317045211792\n",
      "Thriller\n",
      "0.6335912704467773\n",
      "\n",
      "Actual:\n",
      "Horror\n",
      "Thriller\n",
      "\n",
      "Plot:\n",
      "Five youngsters are on their way back home. They stop the car for no reason and they begin to disappear, one by one. Some of them leave a trail of blood behind. There's a sick maniac around who just wants torture them, killing them and have a little fun with their misery. The camera they bring with them records everything. Until the end... \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set, 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 18646\n",
      "Predicted:\n",
      "HAN:\n",
      "Comedy\n",
      "0.9463046073913575\n",
      "Romance\n",
      "0.6670489311218262\n",
      "USE + HAN:\n",
      "Comedy\n",
      "0.7440277099609375\n",
      "Drama\n",
      "0.5314347267150878\n",
      "Romance\n",
      "0.686713695526123\n",
      "USE + FC:\n",
      "Comedy\n",
      "0.5145308971405029\n",
      "Drama\n",
      "0.6171409606933593\n",
      "\n",
      "Actual:\n",
      "Comedy\n",
      "\n",
      "Plot:\n",
      "After 30 years together, Henry's parents are finally getting married. As the responsible oldest son he wants everything to be perfect on their big day. But his youngest brother Tom can barely stay conscious and George (the middle brother) can't stop thinking about his ex-wife long enough to focus on the wedding. To complicate things further, Rhonda, the wedding coordinator chooses today to tell George that she's loved him for 10 years. Can Henry pull everyone together in time for the wedding? And what will his parents do if he can't? \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 17481\n",
      "Predicted:\n",
      "HAN:\n",
      "Drama\n",
      "0.9336017608642578\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.8640570640563965\n",
      "Family\n",
      "0.5481149196624756\n",
      "USE + FC:\n",
      "Drama\n",
      "0.8474091529846192\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "Romance\n",
      "\n",
      "Plot:\n",
      "Eric, an American-born Asian college graduate, must steal a younger student's identity in order to take back the one last year of baseball eligibility that he lost when his dad passed away. Monica, his Coach's All-American co-ed daughter, must ascertain her own questionable identity in order to come to terms with the years of abuse that she suffered after her mother passed away. Through cultural, ethical, and generational challenges, Eric and Monica hide their true selves from one another as long as they can. But can he ground her increasingly self- destructive behavior back to reality in order to avoid even further traumatic consequences? \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 13534\n",
      "Predicted:\n",
      "HAN:\n",
      "Comedy\n",
      "0.5201759815216065\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.0821851372718811\n",
      "USE + FC:\n",
      "Drama\n",
      "0.06991367936134338\n",
      "\n",
      "Actual:\n",
      "Comedy\n",
      "\n",
      "Plot:\n",
      "Don Herculano, un viejo verde, decide enamorarse de la esposa del molinero del pueblo, una muchacha joven y hermosa. El molinero al enterarse, planea vengarse y comienza a seducir a la esposa del viejo para colocarlo en ridiculo frente al pueblo. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 2630\n",
      "Predicted:\n",
      "HAN:\n",
      "Drama\n",
      "0.9434134483337402\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.7769200325012207\n",
      "USE + FC:\n",
      "Drama\n",
      "0.7935019493103027\n",
      "Romance\n",
      "0.5418951988220215\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "Romance\n",
      "\n",
      "Plot:\n",
      "After some years of tension, Richard begins a sexual relationship with his sister Natalie, who is now married. The relationship between Richard and Natalie proves dangerously obsessional. Their private intensity (& working class origins) contrast with the middle-class, inhibited, stuffy public scenes we see in the Richmond world into which Natalie has moved with her marriage. As the guilt and intensity of the siblings increases we seem to be heading for disaster, especially when Natalie's husband Sinclair finds out. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 2971\n",
      "Predicted:\n",
      "HAN:\n",
      "Horror\n",
      "0.952242088317871\n",
      "Thriller\n",
      "0.7569305419921875\n",
      "USE + HAN:\n",
      "Horror\n",
      "0.9257663726806641\n",
      "Thriller\n",
      "0.6404680728912353\n",
      "USE + FC:\n",
      "Horror\n",
      "0.8416253089904785\n",
      "Thriller\n",
      "0.5259698867797852\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "Thriller\n",
      "\n",
      "Plot:\n",
      "When his publishers tell him he owes them one last book, Edward Sheehan (Michael Madsen) has little choice but to isolate himself in a cabin on a New England lake. Though his fans have come to expect truly terrifying stories from the mind of Edward, they are unaware of the morbid method he takes to get them on paper. He fears the inner depths he'll have to reach in order to make it to the final chapter. With only himself and his demons, he slowly drives himself crazy as he creates the most sinister book of his career: Gabrielle. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 11370\n",
      "Predicted:\n",
      "HAN:\n",
      "Drama\n",
      "0.6258431434631347\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.08882547616958618\n",
      "USE + FC:\n",
      "Drama\n",
      "0.06462981700897216\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "'Mahendra Singh Dhoni' (qv) is a goalkeeper in school football team.Bannerjee a school cricket coach asks him to join his cricket team and practice daily with him for two hours time passes and he becomes a big state level cricketer but for a long time his luck doesn't favor him to become a member of Indian Cricket team.Dhoni takes up a job in Indian Railways as a ticket checker and plays cricket for the railways after long wait of 4 years he gets selected in Indian Cricket team and turns to be one of the best cricketing captains in history of Indian Cricket. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1774\n",
      "Predicted:\n",
      "HAN:\n",
      "Crime\n",
      "0.9577314376831054\n",
      "Drama\n",
      "0.9195327758789062\n",
      "USE + HAN:\n",
      "Crime\n",
      "0.5950496196746826\n",
      "Drama\n",
      "0.6737801551818847\n",
      "USE + FC:\n",
      "Crime\n",
      "0.5614498615264892\n",
      "Drama\n",
      "0.6798087596893311\n",
      "\n",
      "Actual:\n",
      "Action\n",
      "Adventure\n",
      "Crime\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "Tim Kerry, a cop on the beat in Hell's Kitchen, welcomes his grown son Ritzy home after a two-year prison term. Ritzy's former friends and his ex-girl Julia join with Tim in hoping he'll go straight. But local gang leader Claire Morelli (the former boss's widow) knows of Ritzy's talents from other ex-cons, and makes him an offer hard to refuse. A chain of double-crosses seems to make tragedy inevitable... \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 16315\n",
      "Predicted:\n",
      "HAN:\n",
      "Comedy\n",
      "0.9959463119506836\n",
      "USE + HAN:\n",
      "Comedy\n",
      "0.8002121925354004\n",
      "USE + FC:\n",
      "Comedy\n",
      "0.795903205871582\n",
      "\n",
      "Actual:\n",
      "Comedy\n",
      "\n",
      "Plot:\n",
      "Asaf Rotem is an aspiring producer, waiting for his breakthrough. For his meeting with Moshe, a local TV station manager, he prepared notes about a great show with pretentious concepts. But Moshe, who's interested in ratings rather than notions, already made other plans: He teams Asaf with Yaniv and Aharon. The first is an inept wedding photographer, the other is a lame soundman, and neither one with real experience. So the trio is on to make \"Star Trek\" - interviews with celebrities at their own homes. In each segment they'll somehow manage to ruin either the production, or their relationship with the interviewee (and usually both). \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 2288\n",
      "Predicted:\n",
      "HAN:\n",
      "Comedy\n",
      "0.8818771362304687\n",
      "Drama\n",
      "0.7000470161437988\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.5838805675506592\n",
      "Romance\n",
      "0.5482110500335693\n",
      "USE + FC:\n",
      "Drama\n",
      "0.6637689590454101\n",
      "\n",
      "Actual:\n",
      "Comedy\n",
      "\n",
      "Plot:\n",
      "'A Sensitive Subject' is a comedic short film that examines the importance of being honest and forthright in relationships. It is the story of a neurotic man attempting to ignore his anxieties as he tries sweep a beautiful young woman off of her feet. Allen knows that everyone has a past though he does his best to deny it. While on a date with Claire, he discovers that some people have more of a past than others. Terrified with what he has learned, Allen begins to question whether or not he is 'qualified ' to continue seeing Claire. Armed with bad advice and a bag of prophylactics, Allen decides to pursue a romance with Claire even if it means coming face to face with what and who he fears the most. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 206\n",
      "Predicted:\n",
      "HAN:\n",
      "Drama\n",
      "0.6056915760040283\n",
      "Family\n",
      "0.7499758720397949\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.5312854290008545\n",
      "Family\n",
      "0.6008799076080322\n",
      "USE + FC:\n",
      "Drama\n",
      "0.6729323387145996\n",
      "Family\n",
      "0.6228350639343262\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "Based on the real life journey of Joy Womack, JOY is the story of a young ballerina from Texas who realizes the only way to achieve her dream of becoming the best is to make a leap of faith and move to Moscow where she becomes the first American to train at the famed Bolshoi Ballet Academy. At the age of fifteen and with nothing but the support of her family and her own determination, Joy makes the courageous move to Russia by herself and learns to navigate the intense politics, difficult language and unique way of life that come with her new home. As her belief in herself is continually tested, Joy's faith and her passion for dance propel her to center stage and allow her to find the hidden beauty of a mysterious new country and an inner strength she never knew she had. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 22335\n",
      "Predicted:\n",
      "HAN:\n",
      "Drama\n",
      "0.5438747882843018\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.5837928771972656\n",
      "USE + FC:\n",
      "Comedy\n",
      "0.5586716651916503\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "Riccardo is a child with Down syndrome with a burning passion for the theatre. Each afternoon, with the collusion of Mr. De Angelis, he sneaks into the small-town theatre in order to witness rehearsals featuring Mattia as he prepares for a show inspired by Marcel Marceau. The young Mattia, however, is hiding a secret that no one suspects. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 2607\n",
      "Predicted:\n",
      "HAN:\n",
      "Drama\n",
      "0.5823805809020997\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.6484965324401856\n",
      "USE + FC:\n",
      "Drama\n",
      "0.6357914447784424\n",
      "\n",
      "Actual:\n",
      "Drama\n",
      "\n",
      "Plot:\n",
      "Paco Chavez's life is careless and charming. It's a life of illicit drugs and a forbidden love affair he carries with Lucia, his former high school sweetheart, now married to another man. One night, Paco and his younger brother Luis, enter their parent's home to steal a porcelain horse to pawn in order to score more drugs. Their father catches them and a fight ensues. The consequences of that fight will haunt both brothers forever. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 11170\n",
      "Predicted:\n",
      "HAN:\n",
      "Action\n",
      "0.726656150817871\n",
      "Crime\n",
      "0.7168906211853028\n",
      "Thriller\n",
      "0.6963237762451172\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.516370439529419\n",
      "Thriller\n",
      "0.6439319610595703\n",
      "USE + FC:\n",
      "Drama\n",
      "0.588694715499878\n",
      "\n",
      "Actual:\n",
      "Thriller\n",
      "\n",
      "Plot:\n",
      "Money makes the world go round for Jett Gavallon, a high-tech entrepreneur who's on the brink of bringing a Russian telecom startup to market with an IPO worth billions. But when his best friend and second-in-command disappears after Gavallon sends him to Moscow to make sure the new company is on the up and up, Jett begins to have second thoughts, which are exactly what his Russian partners can't afford. Beset by an FBI task force looking into Mercury Broadband's financing by Russian mobsters, rumors of fraud being circulated by a Drudge-like online financial gossip columnist, and the discovery that his former lover is not who he thought she was, Jett puts his fortune on the line in a desperate attempt to save his company, and ultimately, his life. An exciting, fast-paced adventure by an author who puts his experience in international banking to work in the service of this carefully plotted thriller. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1600\n",
      "Predicted:\n",
      "HAN:\n",
      "Action\n",
      "0.5134472370147705\n",
      "Adventure\n",
      "0.5167732238769531\n",
      "Drama\n",
      "0.686297607421875\n",
      "USE + HAN:\n",
      "Drama\n",
      "0.50898756980896\n",
      "Family\n",
      "0.5164819717407226\n",
      "USE + FC:\n",
      "Family\n",
      "0.6193256855010987\n",
      "\n",
      "Actual:\n",
      "Comedy\n",
      "\n",
      "Plot:\n",
      "Jimmy Bancroft, a fighter pilot, who is recovering from injuries sustained during the Battle of Britain, and his nurse Hazel Broome, come across a pair of rare birds nestling in a field. After a run in with the army, and a couple of thieves, they, with the cooperation of the village people and the Ornithology Society, help the eggs to hatch. A wonderful look at life in a small village, during World War II. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 18132\n",
      "Predicted:\n",
      "HAN:\n",
      "Action\n",
      "0.8316841125488281\n",
      "Sci-Fi\n",
      "0.9772924423217774\n",
      "USE + HAN:\n",
      "Sci-Fi\n",
      "0.9067022323608398\n",
      "USE + FC:\n",
      "Sci-Fi\n",
      "0.6863724708557128\n",
      "\n",
      "Actual:\n",
      "Sci-Fi\n",
      "\n",
      "Plot:\n",
      "It is the year 2037 and the world has seen a decade of global peace and prosperity. One day, one moment changed everything. What was once a beaming society, is now no more than ash and rumble. The nuclear holocaust spared a few, Dallas and his wife Mary were among those that survived the end of the world. Just as this husband lost the life he knew, his only companion in this Hell on earth is taken by men that police the New World Order, where debauchery and mayhem runs rapid. Dallas sets out on a quest to rescue his wife while traversing an apocalyptic landscape in the wild wild West, composed of blood thirsty cannibals and radiation infected sub-humans. What lies before him is an impossible task, but he is determined and willing to do anything and everything to be by his wife's side. This journey will take him to his limits, this journey will take him to Dirt City. \n"
     ]
    }
   ],
   "source": [
    "compareEnsemblePred(HAN_sum, USE_HAN_sum, USE_FC_sum, y_test, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average cosine similarity between our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90159565\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = np.sum(HAN_sum*USE_HAN_sum, axis = 1)/(np.linalg.norm(HAN_sum, axis = 1)*np.linalg.norm(USE_HAN_sum, axis = 1))\n",
    "average_cosine_sim = np.average(cosine_sim)\n",
    "print(average_cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8868555\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = np.sum(HAN_sum*USE_FC_sum, axis = 1)/(np.linalg.norm(HAN_sum, axis = 1)*np.linalg.norm(USE_FC_sum, axis = 1))\n",
    "average_cosine_sim = np.average(cosine_sim)\n",
    "print(average_cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9643225\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = np.sum(USE_FC_sum*USE_HAN_sum, axis = 1)/(np.linalg.norm(USE_FC_sum, axis = 1)*np.linalg.norm(USE_HAN_sum, axis = 1))\n",
    "average_cosine_sim = np.average(cosine_sim)\n",
    "print(average_cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ensemble of our ensembles and look at our evaluation metrics for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indiv_class_scores_ensemble_of_ensembles(HAN_sum, USE_HAN_sum, USE_FC_sum, y_true, threshold):\n",
    "    \"\"\"y_pred is a dictionary of predictions while\n",
    "        y_true is an array of predictions for the dev or test set\"\"\"\n",
    "    \n",
    "    list_of_predictions = []\n",
    "    \n",
    "    for preds in list(HAN_sum.values()):\n",
    "        max_pred = np.array(preds.max(axis = 1)).reshape(-1,1)\n",
    "    \n",
    "        #Cant predict 0 for all classes - pick max val as label if all preds below threshold\n",
    "        for i in range(len(max_pred)): \n",
    "            if max_pred[i] > threshold:\n",
    "                max_pred[i] = threshold\n",
    "            else:\n",
    "                pass\n",
    "        list_of_predictions.append(preds >= max_pred)\n",
    "    \n",
    "    voted_preds = sum(list_of_predictions)\n",
    "    \n",
    "    max_voted_preds = np.array(voted_preds.max(axis = 1)).reshape(-1,1)\n",
    "    \n",
    "    for i in range(len(max_voted_preds)): \n",
    "        if max_voted_preds[i] > 2.5:\n",
    "            max_voted_preds[i] = 2.5\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    y_pred_HAN = np.where(voted_preds >= max_voted_preds, 1, 0) #3 for yes is needed to predict a class\n",
    "    \n",
    "        \n",
    "    list_of_predictions = []\n",
    "    \n",
    "    for preds in list(USE_HAN_sum.values()):\n",
    "        max_pred = np.array(preds.max(axis = 1)).reshape(-1,1)\n",
    "    \n",
    "        #Cant predict 0 for all classes - pick max val as label if all preds below threshold\n",
    "        for i in range(len(max_pred)): \n",
    "            if max_pred[i] > threshold:\n",
    "                max_pred[i] = threshold\n",
    "            else:\n",
    "                pass\n",
    "        list_of_predictions.append(preds >= max_pred)\n",
    "    \n",
    "    voted_preds = sum(list_of_predictions)\n",
    "    \n",
    "    max_voted_preds = np.array(voted_preds.max(axis = 1)).reshape(-1,1)\n",
    "    \n",
    "    for i in range(len(max_voted_preds)): \n",
    "        if max_voted_preds[i] > 2.5:\n",
    "            max_voted_preds[i] = 2.5\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    y_pred_USE_HAN = np.where(voted_preds >= max_voted_preds, 1, 0) #3 for yes is needed to predict a cla\n",
    "    \n",
    "        \n",
    "    list_of_predictions = []\n",
    "    \n",
    "    for preds in list(USE_FC_sum.values()):\n",
    "        max_pred = np.array(preds.max(axis = 1)).reshape(-1,1)\n",
    "    \n",
    "        #Cant predict 0 for all classes - pick max val as label if all preds below threshold\n",
    "        for i in range(len(max_pred)): \n",
    "            if max_pred[i] > threshold:\n",
    "                max_pred[i] = threshold\n",
    "            else:\n",
    "                pass\n",
    "        list_of_predictions.append(preds >= max_pred)\n",
    "    \n",
    "    voted_preds = sum(list_of_predictions)\n",
    "    \n",
    "    max_voted_preds = np.array(voted_preds.max(axis = 1)).reshape(-1,1)\n",
    "    \n",
    "    for i in range(len(max_voted_preds)): \n",
    "        if max_voted_preds[i] > 2.5:\n",
    "            max_voted_preds[i] = 2.5\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    y_pred_USE_FC = np.where(voted_preds >= max_voted_preds, 1, 0) #3 for yes is needed to predict a cla\n",
    "    \n",
    "    y_pred_sum = (y_pred_HAN + y_pred_USE_HAN + y_pred_USE_FC)\n",
    "    \n",
    "    y_pred = np.where(y_pred_sum >=1.5, 1, 0)\n",
    "    \n",
    "    for i in range(len(mlb.classes_)):\n",
    "        score = precision_score(y_true[:,i], y_pred[:,i])\n",
    "        print(\"The precision for {} is {}\".format(mlb.classes_[i], score))\n",
    "    \n",
    "    for i in range(len(mlb.classes_)):  \n",
    "        score = recall_score(y_true[:,i], y_pred[:,i])\n",
    "        print(\"The recall for {} is {}\".format(mlb.classes_[i], score))\n",
    "        \n",
    "    for i in range(len(mlb.classes_)): \n",
    "        score = f1_score(y_true[:,i], y_pred[:,i])\n",
    "        print(\"The f1 for {} is {}\".format(mlb.classes_[i], score))\n",
    "    \n",
    "    micro_precision = precision_score(y_true, y_pred, average = 'micro')\n",
    "    weighted_macro_precision = precision_score(y_true, y_pred, average = 'weighted')\n",
    "    micro_recall = recall_score(y_true, y_pred, average = 'micro')\n",
    "    weighted_macro_recall = recall_score(y_true, y_pred, average = 'weighted')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average = 'micro')\n",
    "    weighted_macro_f1 = f1_score(y_true, y_pred, average = 'weighted')\n",
    "    print(\"The micro precision is\", micro_precision)\n",
    "    print(\"The weighted macro precision is\", weighted_macro_precision)\n",
    "    print(\"The micro recall is\", micro_recall)\n",
    "    print(\"The weighted macro recall is\", weighted_macro_recall)\n",
    "    print(\"The micro f1 is\", micro_f1)\n",
    "    print(\"The weighted macro f1 is\", weighted_macro_f1)\n",
    "    \n",
    "    num_preds = y_pred.sum(axis = 1)\n",
    "    avg_genre_per_pred = np.average(num_preds)\n",
    "    median_genre_per_pred = np.median(num_preds)\n",
    "    print(\"The average number of genres per movie is\", avg_genre_per_pred)\n",
    "    print(\"The median number of genres per movie is\", median_genre_per_pred)\n",
    "        \n",
    "    #return list_of_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for Action is 0.5368754956383822\n",
      "The precision for Adventure is 0.4700906344410876\n",
      "The precision for Comedy is 0.6975735175960148\n",
      "The precision for Crime is 0.4175392670157068\n",
      "The precision for Drama is 0.7109765329295988\n",
      "The precision for Family is 0.43192692474989125\n",
      "The precision for Horror is 0.5964556962025317\n",
      "The precision for Romance is 0.3910564225690276\n",
      "The precision for Sci-Fi is 0.5929319371727748\n",
      "The precision for Thriller is 0.4762652020400157\n",
      "The recall for Action is 0.557660626029654\n",
      "The recall for Adventure is 0.429123000551572\n",
      "The recall for Comedy is 0.6132222065263455\n",
      "The recall for Crime is 0.5325542570951586\n",
      "The recall for Drama is 0.7988432423237221\n",
      "The recall for Family is 0.49874434957307884\n",
      "The recall for Horror is 0.6097308488612836\n",
      "The recall for Romance is 0.4643620812544547\n",
      "The recall for Sci-Fi is 0.6574746008708273\n",
      "The recall for Thriller is 0.42050571527537234\n",
      "The f1 for Action is 0.547070707070707\n",
      "The f1 for Adventure is 0.4486735870818916\n",
      "The f1 for Comedy is 0.6526838069463239\n",
      "The f1 for Crime is 0.46808510638297873\n",
      "The f1 for Drama is 0.7523531061000521\n",
      "The f1 for Family is 0.46293706293706294\n",
      "The f1 for Horror is 0.6030202201177375\n",
      "The f1 for Romance is 0.4245682632779407\n",
      "The f1 for Sci-Fi is 0.6235375086028906\n",
      "The f1 for Thriller is 0.4466519499632083\n",
      "The micro precision is 0.5964081415458294\n",
      "The weighted macro precision is 0.5995556260978314\n",
      "The micro recall is 0.6249581799933088\n",
      "The weighted macro recall is 0.6249581799933088\n",
      "The micro f1 is 0.6103494751746015\n",
      "The weighted macro f1 is 0.609887535821354\n",
      "The average number of genres per movie is 1.6755828986670234\n",
      "The median number of genres per movie is 2.0\n"
     ]
    }
   ],
   "source": [
    "indiv_class_scores_ensemble_of_ensembles(HAN_preds, USE_HAN_preds, USE_FC_preds, y_test, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
