{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "import unittest\n",
    "import RegexTester\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.callbacks import History, CSVLogger\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data - will not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('genres.csv', header=None, encoding = \"ISO-8859-1\")\n",
    "data2 = pd.read_csv('genres2.csv', header=None, encoding = \"ISO-8859-1\")\n",
    "data3 = pd.read_csv('genres3.csv', header=None, encoding = \"ISO-8859-1\")\n",
    "\n",
    "data = pd.concat([data1, data2, data3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "data.groupby(1)[0].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmetizing Function\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize w/lemmetization AFTER removing stopwords - TOKENIZER 1\n",
    "def tokenize(plots, lemmatize = False):\n",
    "    \n",
    "    def re_sub(pattern, replace):\n",
    "        return re.sub(pattern, replace, plots)\n",
    "    \n",
    "    plots = plots.lower() #lowercase\n",
    "    plots = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,/.\\d]*\", \"<number>\") #generic tag for numbers\n",
    "    plots = re_sub(r\"([!?.]){2,}\", r\"\\1\") #Convert multiple punctuations to the last punctuation mark\n",
    "    plots = plots.replace('-',' ') #separating hyphenated words\n",
    "    plots = plots.replace('_','') #remove underscores\n",
    "    plots = re_sub(r'(?<!\\w)([a-zA-Z])\\.', r'\\1') #remove periods from abbreviations\n",
    "    plots = re_sub('[^\\w\\s\\.\\<>\\?\\!]','') #remove punctuation besides sentence completers and <> for generic number\n",
    "    plots = plots.lower().split()\n",
    "    output = list(itertools.chain(*[re.split(r'([^\\w<>])', x) for x in plots if x not in stop])) #split sentence enders and remove stopwords\n",
    "    output = [item for item in output if item != '']\n",
    "    \n",
    "    if lemmatize:\n",
    "        output_lem = nltk.pos_tag(output)\n",
    "        return [lemmatizer.lemmatize(x[0], pos = nltk2wn_tag(x[1])) for x in output_lem]\n",
    "    else:\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize w/lemmetization BEFORE removing stopwords - TOKENIZER 2\n",
    "def tokenize(plots, lemmatize = False):\n",
    "    \n",
    "    def re_sub(pattern, replace):\n",
    "        return re.sub(pattern, replace, plots)\n",
    "    \n",
    "    plots = plots.lower() #lowercase\n",
    "    plots = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,/.\\d]*\", \"<number>\") #generic tag for numbers\n",
    "    plots = re_sub(r\"([!?.]){2,}\", r\"\\1\") #Convert multiple punctuations to the last punctuation mark\n",
    "    plots = plots.replace('-',' ') #separating hyphenated words\n",
    "    plots = re_sub(r'(?<!\\w)([a-zA-Z])\\.', r'\\1') #remove periods from abbreviations\n",
    "    plots = re_sub('[^\\w\\s\\.\\<>\\?\\!]','') #remove punctuation besides sentence completers and <> for generic number\n",
    "    plots = plots.lower().split()\n",
    "    if lemmatize:\n",
    "        plots = nltk.pos_tag(plots)\n",
    "        plots = [lemmatizer.lemmatize(x[0], pos = nltk2wn_tag(x[1])) for x in plots]\n",
    "    output = list(itertools.chain(*[re.split(r'([^\\w<>])', x) for x in plots if x not in stop])) #split sentence enders and remove stopwords\n",
    "    output = [item for item in output if item != '']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(test_module, test_names, reload=True):\n",
    "    import unittest\n",
    "    if reload:\n",
    "        import importlib\n",
    "        importlib.reload(test_module)\n",
    "    unittest.TextTestRunner(verbosity=2).run(unittest.TestLoader().loadTestsFromNames(test_names, test_module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests(RegexTester, [\"NumberRegex\"])\n",
    "run_tests(RegexTester, [\"RepeatedPunctuationRegex\"])\n",
    "run_tests(RegexTester, [\"HyphenRegex\"])\n",
    "run_tests(RegexTester, [\"AbbreviationRegex\"])\n",
    "run_tests(RegexTester, [\"PunctuationRemovalRegex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Tokenizer 1\n",
    "print(tokenize(\"Hello, MYself dear $$20-30 hello? 2,00.0 A.J.A what??\"))\n",
    "print(\"\\n\")\n",
    "print(full_data[\"plots\"][0])\n",
    "print(\"\\n\")\n",
    "print(tokenize(full_data[\"plots\"][0]))\n",
    "print(\"\\n\")\n",
    "print(tokenize(full_data[\"plots\"][0], lemmatize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZER 2\n",
    "print(tokenize(full_data[\"plots\"][0], lemmatize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Tokenizer to Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_data[\"plots_processed\"] = full_data[\"plots\"].apply(lambda row: tokenize(row, lemmatize=True)) #Tokenizer 1\n",
    "end = time.time()\n",
    "print(\"Total Time to tokenize plots:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"plots_processed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbed(file):\n",
    "    start = time.time()\n",
    "    print(\"Loading Embeddings\")\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    status_every = 100000\n",
    "    for i, line in enumerate(f):\n",
    "        if i%status_every == 0:\n",
    "            print('Processing line {:,}'.format(i))\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",'{:,}'.format(len(model)),\" words loaded!\")\n",
    "    end = time.time()\n",
    "    print(\"Total Time to load embeddings:\", end - start, \"seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOO LARGE TO PUSH TO GIT, DOWNLOAD SEPARATLEY FROM https://github.com/stanfordnlp/GloVe\n",
    "glove_dir = './glove.6B/'\n",
    "glove_filename = 'glove.6B.300d.txt'\n",
    "glove_fullpath = glove_dir + glove_filename\n",
    "glove_dd = loadEmbed(glove_fullpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can also use assignment 2 code for glove embeddings since we are using the same embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT - CREATE PICKLE OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pkl_file(obj, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pkl.dump(obj, file)\n",
    "\n",
    "\n",
    "embedding_vocab = list(glove_dd.keys())\n",
    "create_pkl_file(glove_dd, 'glove_embeddings.pickle')\n",
    "create_pkl_file(embedding_vocab, 'embedding_vocab.pickle')\n",
    "create_pkl_file(full_data, 'full_data_w_processed_plots_lemmatized.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pkl.load( open(\"full_data_w_processed_plots_lemmatized.pickle\", \"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_plot(plot):\n",
    "    return np.array([glove_dd.get(word, glove_dd.get(\"unk\")) for word in plot]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_plot(full_data[\"plots_processed\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
