{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'core' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5341e4f0be2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_to_word_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mpython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;31m# pylint: enable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'core' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.callbacks import History, CSVLogger\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in train and test txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genres = open(\"train_genres.txt\", encoding=\"utf-8\").read().split('\\n')\n",
    "train_plots = open(\"train_plots.txt\", encoding=\"utf-8\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({\"genres\": train_genres, \"plots\": train_plots})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[4][\"genres\"].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genres = open(\"test_genres.txt\", encoding=\"utf-8\").read().split('\\n')\n",
    "test_plots = open(\"test_plots.txt\", encoding=\"utf-8\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({\"genres\": test_genres, \"plots\": test_plots})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat dataframes\n",
    "full_data = pd.concat([train_data, test_data], ignore_index= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRIEF EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert genres to list for easier analysis\n",
    "def list_genres(row):\n",
    "    return row[\"genres\"].split(\" \")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"list_genres\"] = full_data.apply(lambda row: list_genres(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count num of movies per genre\n",
    "def dict_count(row):\n",
    "    global count_dict\n",
    "    for genre in row[\"list_genres\"]:\n",
    "        count_dict[genre] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN ONLY ONCE!!!\n",
    "count_val_series = full_data.apply(lambda row: dict_count(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count num of movies per genre\n",
    "for key,val in count_dict.items():\n",
    "    print(\"{:0.2f}% of the movies are {}\".format(100*val/len(full_data), key))\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1) \n",
    "ax.bar(range(len(count_dict.keys())), list(count_dict.values()))\n",
    "ax.set_xticks(range(len(count_dict.keys())))\n",
    "ax.set_xticklabels(list(count_dict.keys()), rotation='vertical', fontsize=18)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of genres per movie\n",
    "genre_labels = full_data[\"list_genres\"].str.len()\n",
    "print(\"Avg num of genres:\", np.mean(genre_labels))\n",
    "print(\"Median num of genres:\", np.median(genre_labels))\n",
    "print(\"Max number of genres:\", np.max(genre_labels))\n",
    "print(\"Min number of genres:\", np.min(genre_labels))\n",
    "\n",
    "\n",
    "plt.hist(genre_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSOLIDATE Genre Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted(count_dict.items(), key=operator.itemgetter(1))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [\"Animation\", \"Fantasy\", \"Mystery\", \n",
    "          \"Biography\", \"Music\", \"History\", \"War\", \"Sport\", \"Western\", \"Musical\"]\n",
    "\n",
    "def remove_genres(genres):\n",
    "    return [x for x in genres if x not in remove]\n",
    "\n",
    "full_data[\"list_genres_consol\"] = full_data[\"list_genres\"].apply(lambda row: remove_genres(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data[full_data.astype(str)[\"list_genres_consol\"] != \"[]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = [\"comedy\", \"drama\", \"adventure\", \"action\", \"thriller\", \"family\", \"romance\", \"horror\", \"crime\", \"sci-fi\", \"scifi\"]\n",
    "for key in remove:\n",
    "    if key in count_dict:\n",
    "        del count_dict[key]\n",
    "\n",
    "for key,val in count_dict.items():\n",
    "    print(\"{:0.2f}% of the movies are {}\".format(100*val/len(full_data), key))\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1) \n",
    "ax.bar(range(len(count_dict.keys())), list(count_dict.values()))\n",
    "ax.set_xticks(range(len(count_dict.keys())))\n",
    "ax.set_xticklabels(list(count_dict.keys()), rotation='vertical', fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average length of plot summary (characters)\n",
    "avg_num_chars = train_data[\"plots\"].str.len().mean()\n",
    "max_num_chars = train_data[\"plots\"].str.len().max()\n",
    "print(f'The average number of characters in a summary is {avg_num_chars}, max is {max_num_chars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_num_words = train_data[\"plots\"].str.count(\" \").mean() + 1\n",
    "max_num_words = train_data[\"plots\"].str.count(\" \").max() + 1\n",
    "median_num_words = train_data[\"plots\"].str.count(\" \").median() + 1\n",
    "avg_num_periods = train_data[\"plots\"].str.count(\"[\\.\\?!]\").mean()\n",
    "max_num_periods = train_data[\"plots\"].str.count(\"[\\.\\?!]\").max()\n",
    "median_num_periods = train_data[\"plots\"].str.count(\"[\\.\\?!]\").median()\n",
    "print(f'The average number of words in a summary is {avg_num_words}, max is {max_num_words}, median is {median_num_words}')\n",
    "print(f'The average number of sentences in a summary is {avg_num_periods}, max is {max_num_periods}, median is {median_num_periods}')\n",
    "print(avg_num_words/avg_num_periods)\n",
    "print(median_num_words/median_num_periods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"plots\"].str.count(\"\\n\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative way to calculate plot length\n",
    "print(train_data[\"plots\"].str.lower().str.split().str.len().mean()) #Avg words\n",
    "print(train_data[\"plots\"].str.lower().str.split().str.len().max()) #Max words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_words = Counter(\" \".join(train_data[\"plots\"]).split(\" \"))\n",
    "print(len(dict_words.keys())) #should be number of unique words\n",
    "print(sum(dict_words.values())) #should be total number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmetizing Function\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize w/lemmetization AFTER removing stopwords \n",
    "#https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "def tokenize(plot, stop_words, lemmatize = False):\n",
    "    \n",
    "    def re_sub(pattern, replace):\n",
    "        return re.sub(pattern, replace, plot)\n",
    "    \n",
    "    plot = plot.lower() #lowercase\n",
    "    plot = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,/.\\d]*\", \"DG\") #generic tag for numbers\n",
    "    plot = re_sub(r\"([!?.]){2,}\", r\"\\1\") #Convert multiple punctuations to the last punctuation mark\n",
    "    plot = plot.replace('-',' ') #separating hyphenated words\n",
    "    plot = plot.replace('_','') #remove underscores\n",
    "    plot = re_sub(r'(?<!\\w)([a-zA-Z])\\.', r'\\1') #remove periods from abbreviations\n",
    "    plot = re_sub('[^\\w\\s\\.\\?\\!\\']','') #remove punctuation besides sentence completers and apostrophes\n",
    "    sentences = nltk.sent_tokenize(plot)\n",
    "    words = list(map(nltk.word_tokenize, sentences))\n",
    "    words = [[x for x in w if not x in stop_words] for w in words]\n",
    "    words = [[x if not x in keep else \"genre\" for x in w] for w in words]\n",
    "    if lemmatize:\n",
    "        output_lem = [nltk.pos_tag(w) for w in words]\n",
    "        return [[lemmatizer.lemmatize(x[0], pos = nltk2wn_tag(x[1])) for x in w] for w in output_lem]\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_USE(plot):\n",
    "    def re_sub(pattern, replace):\n",
    "        return re.sub(pattern, replace, plot)\n",
    "    \n",
    "    plot = plot.lower() #lowercase\n",
    "    plot = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,/.\\d]*\", \"DG\") #generic tag for numbers\n",
    "    plot = re_sub(r\"([!?.]){2,}\", r\"\\1\") #Convert multiple punctuations to the last punctuation mark\n",
    "    plot = plot.replace('-',' ') #separating hyphenated words\n",
    "    plot = plot.replace('_','') #remove underscores\n",
    "    plot = re_sub(r'(?<!\\w)([a-zA-Z])\\.', r'\\1') #remove periods from abbreviations\n",
    "    plot = re_sub('[^\\w\\s\\.\\?\\!\\']','') #remove punctuation besides sentence completers and apostrophes\n",
    "    plot = plot.split()\n",
    "    plot = [x if x not in keep else \"genre\" for x in plot]\n",
    "    return \" \".join(plot)\n",
    "\n",
    "def tokenize_USE_sentence(plot):\n",
    "    return nltk.sent_tokenize(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = \"Hello my name is sharad 50 years. I like actionable comedy alot!\"\n",
    "tokenize_USE(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_data[\"USE_tokens\"] = full_data.apply(lambda row: tokenize_USE(row['plots']), axis=1)\n",
    "end = time.time()\n",
    "print(\"Total Time to tokenize plots:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_data['tokenized_words'] = full_data.apply(lambda row: tokenize(row['plots'], stop, lemmatize = True), axis=1)\n",
    "end = time.time()\n",
    "print(\"Total Time to tokenize plots:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['flattened_tokens'] = full_data.apply(lambda l: [item for sublist in l['tokenized_words'] for item in sublist], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "full_data[\"USE_tokens_sentences\"] = full_data.apply(lambda row: tokenize_USE_sentence(row['USE_tokens']), axis=1)\n",
    "end = time.time()\n",
    "print(\"Total Time to tokenize plots:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(full_data[\"list_genres_consol\"])\n",
    "full_data[\"binarized_labels\"] = labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle data/Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_pickle(\"./full_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ IN UNIVERSAL SENTENCE ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_data = pd.read_pickle(\"./local/full_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-hub in /home/akhil.patel1896/anaconda3/lib/python3.6/site-packages (0.3.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /home/akhil.patel1896/anaconda3/lib/python3.6/site-packages (from tensorflow-hub) (1.11.0)\r\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/akhil.patel1896/anaconda3/lib/python3.6/site-packages (from tensorflow-hub) (1.16.1)\r\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/akhil.patel1896/anaconda3/lib/python3.6/site-packages (from tensorflow-hub) (3.6.1)\r\n",
      "Requirement already satisfied: setuptools in /home/akhil.patel1896/anaconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow-hub) (39.1.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0401 21:27:09.933175 140405303584576 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time to import sentence embeddings: 73.64054298400879 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "end = time.time()\n",
    "print(\"Total Time to import sentence embeddings:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time to embed entire plots with USE: 607.6711592674255 seconds\n"
     ]
    }
   ],
   "source": [
    "#EMBED ENTIRE PLOTS\n",
    "start = time.time()\n",
    "messages = np.array(full_data[\"USE_tokens\"])\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "  message_embeddings = session.run(embed(messages))\n",
    "\n",
    "#   for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "#     print(\"Message: {}\".format(messages[i]))\n",
    "#     print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "#     message_embedding_snippet = \", \".join(\n",
    "#         (str(x) for x in message_embedding[:3]))\n",
    "#     print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n",
    "end = time.time()\n",
    "print(\"Total Time to embed entire plots with USE:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"USE_token_plot_embeddings\"] = message_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time to pad and truncate plots for USE: 0.2917203903198242 seconds\n"
     ]
    }
   ],
   "source": [
    "#TRUNCATE AND PAD NUM SENTENCES FOR USE_SENTENCE_LEVEL_ENCODING\n",
    "MAX_SENT = 5\n",
    "def truncate_and_pad_plots(sentences, max_sent):\n",
    "    if len(sentences) < max_sent:\n",
    "        return sentences + [\"EMPTY\"] * (max_sent - len(sentences))\n",
    "    elif len(sentences) > max_sent:\n",
    "        return sentences[0:max_sent]\n",
    "    else:\n",
    "        return sentences\n",
    "\n",
    "start = time.time() \n",
    "full_data[\"USE_tokens_sentences_padded\"] = full_data[\"USE_tokens_sentences\"].apply(lambda row: \n",
    "                                                                                   truncate_and_pad_plots(row, \n",
    "                                                                                                          MAX_SENT))\n",
    "end = time.time()\n",
    "print(\"Total Time to pad and truncate plots for USE:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time to embed individual sentences in the plots with USE: 1504.7398443222046 seconds\n"
     ]
    }
   ],
   "source": [
    "#EMBED INDIVID SENTENCES\n",
    "MAX_SENT = 5\n",
    "start = time.time()\n",
    "messages = tf.reshape(np.stack(full_data[\"USE_tokens_sentences_padded\"]), [-1]) #Make 1 dimensional for tf_HUB, will reshape later\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "  message_embeddings = session.run(embed(messages))\n",
    "\n",
    "#   for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "#     print(\"Message: {}\".format(messages[i]))\n",
    "#     print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "#     message_embedding_snippet = \", \".join(\n",
    "#         (str(x) for x in message_embedding[:3]))\n",
    "#     print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total Time to embed individual sentences in the plots with USE:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_sent_embeddings = message_embeddings.reshape(len(full_data), MAX_SENT, 512)\n",
    "full_data[\"USE_token_sentence_embeddings\"] = full_data_sent_embeddings.tolist()\n",
    "np.save(\"USE_token_sentence_embeddings.npy\" , message_embeddings.reshape(len(full_data), MAX_SENT, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_pickle(\"./full_data.pkl\")\n",
    "USE_sent_embed = np.load(\"USE_token_sentence_embeddings.npy\")\n",
    "full_data[\"USE_token_sentence_embeddings\"] = USE_sent_embed.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over tokenized words and create dictionaries that keep track of number of tokens, length of sentences, and sentences per plot summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "sent_per_summary_dict = {}\n",
    "word_per_sent_dict = {}\n",
    "rows = len(full_data['tokenized_words'])\n",
    "print(rows)#number of plot summaries\n",
    "for i in range(len(full_data['tokenized_words'])):\n",
    "    length = len(full_data['tokenized_words'][i])\n",
    "    if length in sent_per_summary_dict:\n",
    "        sent_per_summary_dict[length] += 1\n",
    "    else:\n",
    "        sent_per_summary_dict[length] = 1\n",
    "    for j in range(length):\n",
    "        word_count = len(full_data['tokenized_words'][i][j])\n",
    "        if word_count in word_per_sent_dict:\n",
    "            word_per_sent_dict[word_count] += 1\n",
    "        else:\n",
    "            word_per_sent_dict[word_count] = 1\n",
    "        for word in full_data['tokenized_words'][i][j]:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_dict.keys())) #should be number of unique words\n",
    "print(sum(word_dict.values())) #should be total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "twoOrOne = 0\n",
    "for value in word_dict.values():\n",
    "    if value == 1:\n",
    "        count +=1\n",
    "    if value <3:\n",
    "        twoOrOne +=1\n",
    "print(len(word_dict.keys()) - count) # words that appear more than once\n",
    "print(len(word_dict.keys()) - twoOrOne) # words that appear more than twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_per_sent_dict.keys())) #should be number of unique sentence lengths\n",
    "print(sum(word_per_sent_dict.values())) #should be number of sentences in all plots\n",
    "print(max(word_per_sent_dict.keys())) #should be largest sentence length\n",
    "total = 0\n",
    "weight_sum = 0\n",
    "for key, value in word_per_sent_dict.items():\n",
    "    total += value\n",
    "    weight_sum += key*value\n",
    "print(weight_sum/total) #should be average sentence length\n",
    "#print(word_per_sent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sent_per_summary_dict.keys())) #should be number of unique sentence lengths per summary\n",
    "print(max(sent_per_summary_dict.keys())) #should be highest amount of sentences per summary\n",
    "total = 0\n",
    "weight_sum = 0\n",
    "for key, value in sent_per_summary_dict.items():\n",
    "    total += value\n",
    "    weight_sum += key*value\n",
    "print(weight_sum/total) #should be average sentence count per summary\n",
    "#print(sent_per_summary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings\n",
    "embeddings_index = {}\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create average word vector. This will later be used in place of unknown words\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        pass\n",
    "n_vec = i + 1\n",
    "hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "average_vec = np.mean(vecs, axis=0)\n",
    "#print(average_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above token analysis, we set the following hyperparameters at these initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 15 #Gets ~80% of sentences\n",
    "MAX_SENTS = 5 #Gets ~80% of summaries\n",
    "MAX_NB_WORDS = 80000 #Eliminates words seen two or fewer times\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(full_data['flattened_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(full_data), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "doc_lst = []\n",
    "\n",
    "# keep the MAX_NB_WORDS most frequent words and replace the rest with 'UNK'\n",
    "# truncate to the first MAX_SENTS sentences per doc and MAX_SENT_LENGTH words per sentence\n",
    "\n",
    "for summary_num, row in full_data.iterrows():\n",
    "    for sent_num, sent in enumerate(row['tokenized_words']):\n",
    "        if sent_num < MAX_SENTS:\n",
    "            word_num = 0\n",
    "            words_in_sent = []\n",
    "            for _, word in enumerate(sent):\n",
    "                if word_num < MAX_SENT_LENGTH: \n",
    "                    if (word in tokenizer.word_index) and (tokenizer.word_index[word] < MAX_NB_WORDS):\n",
    "                        data[summary_num, sent_num, word_num] = tokenizer.word_index[word]\n",
    "                        words_in_sent.append(word)\n",
    "                    else:\n",
    "                        data[summary_num, sent_num, word_num] = MAX_NB_WORDS\n",
    "                        words_in_sent.append('UNK')\n",
    "                    word_num = word_num + 1\n",
    "            doc_lst.append(words_in_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leverage our embedding_index dictionary and our word_index to compute our embedding matrix\n",
    "embedding_matrix = np.zeros((MAX_NB_WORDS+1, EMBEDDING_DIM))\n",
    "count = 0\n",
    "unknown =0\n",
    "added =0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "    if embedding_vector is not None and i < MAX_NB_WORDS:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        added+=1\n",
    "    elif i == MAX_NB_WORDS:\n",
    "        # index MAX_NB_WORDS in data corresponds to 'UNK'\n",
    "        embedding_matrix[i] = average_vec #use average vector for unknown\n",
    "        unknown+=1\n",
    "    else:\n",
    "        count +=1\n",
    "print(added) #of the MAX_NB_WORDS most frequent tokens in our corpus, this many have GloVe embeddings\n",
    "print(unknown)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "absent_words = 0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        absent_words += 1\n",
    "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)), '% of total words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load this embedding matrix into an Embedding layer.\n",
    "REG_PARAM = 1e-13\n",
    "l2_reg = regularizers.l2(REG_PARAM)\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            embeddings_regularizer=l2_reg,\n",
    "                            mask_zero = True, #determines whether masking is performed, i.e. whether the layers ignore the padded zeros in shorter documents\n",
    "                            trainable=False) #prevent weights from being updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=MAX_SENT_LENGTH, \n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f\n",
    "# sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sentence_input)\n",
    "# l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "# sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "# review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "# review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "# l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "# preds = Dense(20, activation='softmax')(l_lstm_sent)\n",
    "# model = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Hsankesara/DeepResearch/blob/master/Hierarchical_Attention_Network/attention_with_context.py\n",
    "#https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words level attention model\n",
    "word_input = Input(shape=(MAX_SENT_LENGTH,), dtype='float32')\n",
    "word_sequences = embedding_layer(word_input)\n",
    "word_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)\n",
    "word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)\n",
    "word_att = AttentionWithContext()(word_dense)\n",
    "wordEncoder = Model(word_input, word_att)\n",
    "\n",
    "# Sentence level attention model\n",
    "sent_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='float32')\n",
    "sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n",
    "sent_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)\n",
    "sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)\n",
    "sent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))\n",
    "preds = Dense(20, activation='sigmoid')(sent_att)\n",
    "model = Model(sent_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate training set into train and dev. Roughly 80% of original data is train, 10% dev, 10% test\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'han_food'\n",
    "history = History()\n",
    "csv_logger = CSVLogger('./{0}_{1}.log'.format(fname, REG_PARAM), separator=',', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=False, \n",
    "          callbacks=[history, csv_logger], verbose=2)\n",
    "\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indiv_class_scores(y_true, y_pred, threshold, metric = None):\n",
    "    y_pred = y_pred >= threshold\n",
    "    for i in range(len(mlb.classes_)):\n",
    "        if metric == \"precision\":\n",
    "            score = precision_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        elif metric == \"recall\":\n",
    "            score = recall_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        elif metric == \"f1\":\n",
    "            score = f1_score(y_true[:,i], y_pred[:,i])\n",
    "            print(\"The {} for {} is {}\".format(metric, mlb.classes_[i], score))\n",
    "        else:\n",
    "            return \"Not a valid metric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_class_scores(y_dev, preds, threshold = 0.5, metric = \"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_class_scores(y_dev, preds, threshold=0.5, metric = \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_class_scores(y_dev, preds, threshold= 0.5, metric= \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = preds >= 0.5\n",
    "micro_precision = precision_score(y_dev, y_pred, average = 'micro')\n",
    "weighted_macro_precision = precision_score(y_dev, y_pred, average = 'weighted')\n",
    "micro_recall = recall_score(y_dev, y_pred, average = 'micro')\n",
    "weighted_macro_recall = recall_score(y_dev, y_pred, average = 'weighted')\n",
    "micro_f1 = f1_score(y_dev, y_pred, average = 'micro')\n",
    "weighted_macro_f1 = f1_score(y_dev, y_pred, average = 'weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The micro precision is\", micro_precision)\n",
    "print(\"The weighted macro precision is\", weighted_macro_precision)\n",
    "print(\"The micro recall is\", micro_recall)\n",
    "print(\"The weighted macro recall is\", weighted_macro_recall)\n",
    "print(\"The micro f1 is\", micro_f1)\n",
    "print(\"The weighted macro f1 is\", weighted_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('HAN_10epochs.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
